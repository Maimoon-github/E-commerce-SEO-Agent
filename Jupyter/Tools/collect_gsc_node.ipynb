{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc00288",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-auth -Uq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9519a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client -Uq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabada0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib google-auth -Uq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd92d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Protocol\n",
    "\n",
    "# Google deps:\n",
    "#   pip install google-api-python-client google-auth google-auth-oauthlib\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from google.oauth2 import service_account\n",
    "from google.auth.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Types + small abstractions\n",
    "# -----------------------------\n",
    "\n",
    "SCOPES = (\n",
    "    # \"https://www.googleapis.com/auth/webmasters.readonly\",\n",
    "    \"https://www.googleapis.com/auth/webmasters\"\n",
    ")\n",
    "\n",
    "SearchRow = Dict[str, Any]\n",
    "JsonDict = Dict[str, Any]\n",
    "\n",
    "\n",
    "class SearchConsoleService(Protocol):\n",
    "    \"\"\"Protocol to allow dependency injection/mocking in tests.\"\"\"\n",
    "\n",
    "    def searchanalytics(self) -> Any: ...\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GSCQueryConfig:\n",
    "    property_url: str\n",
    "    window_days: int = 28\n",
    "    search_type: str = \"web\"  # \"web\", \"image\", \"video\", \"news\", \"discover\", \"googleNews\"\n",
    "    row_limit: int = 25000\n",
    "    max_rows_per_report: int = 50000  # practical safety ceiling per segment/report\n",
    "    sleep_between_calls_s: float = 0.2\n",
    "    max_retries: int = 5\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SegmentReport:\n",
    "    \"\"\"Normalized output for one segment (appearance) and one entity type (page/query).\"\"\"\n",
    "    search_appearance: str\n",
    "    rows: List[SearchRow]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Analyze → Decompose\n",
    "# -----------------------------\n",
    "\n",
    "def authenticate_gsc(\n",
    "    *,\n",
    "    auth_mode: str = \"service_account\",\n",
    "    service_account_json_path: Optional[str] = None,\n",
    "    oauth_client_secrets_path: Optional[str] = None,\n",
    "    oauth_token_path: Optional[str] = None,\n",
    ") -> Credentials:\n",
    "    \"\"\"\n",
    "    Authenticate to the Search Console API.\n",
    "\n",
    "    Supported modes:\n",
    "      - service_account: uses a service account JSON key file\n",
    "      - oauth: interactive OAuth flow, stores refresh token\n",
    "\n",
    "    Notes:\n",
    "      - For service accounts, the GSC property must be shared with the service account email.\n",
    "      - For OAuth, this is suitable for local/dev; production typically uses stored tokens.\n",
    "    \"\"\"\n",
    "    auth_mode = auth_mode.strip().lower()\n",
    "\n",
    "    if auth_mode == \"service_account\":\n",
    "        if not service_account_json_path:\n",
    "            service_account_json_path = os.getenv(\"GSC_SERVICE_ACCOUNT_JSON\")\n",
    "\n",
    "        if not service_account_json_path:\n",
    "            raise ValueError(\"service_account_json_path is required (or set GSC_SERVICE_ACCOUNT_JSON).\")\n",
    "\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            service_account_json_path,\n",
    "            scopes=list(SCOPES),\n",
    "        )\n",
    "        return creds\n",
    "\n",
    "    if auth_mode == \"oauth\":\n",
    "        if not oauth_client_secrets_path:\n",
    "            oauth_client_secrets_path = os.getenv(\"GSC_OAUTH_CLIENT_SECRETS\")\n",
    "\n",
    "        if not oauth_client_secrets_path:\n",
    "            raise ValueError(\"oauth_client_secrets_path is required (or set GSC_OAUTH_CLIENT_SECRETS).\")\n",
    "\n",
    "        if not oauth_token_path:\n",
    "            oauth_token_path = os.getenv(\"GSC_OAUTH_TOKEN_PATH\", \"gsc_oauth_token.json\")\n",
    "\n",
    "        creds: Optional[Credentials] = None\n",
    "        if os.path.exists(oauth_token_path):\n",
    "            from google.oauth2.credentials import Credentials as UserCredentials  # type: ignore\n",
    "            creds = UserCredentials.from_authorized_user_file(oauth_token_path, scopes=list(SCOPES))\n",
    "\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        elif not creds or not creds.valid:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(oauth_client_secrets_path, scopes=list(SCOPES))\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "            with open(oauth_token_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(creds.to_json())\n",
    "\n",
    "        assert creds is not None\n",
    "        return creds\n",
    "\n",
    "    raise ValueError(f\"Unsupported auth_mode={auth_mode!r}. Use 'service_account' or 'oauth'.\")\n",
    "\n",
    "\n",
    "def build_gsc_service(creds: Credentials) -> Any:\n",
    "    \"\"\"\n",
    "    Build the googleapiclient Search Console service.\n",
    "    \"\"\"\n",
    "    # discovery cache disabled to avoid file system surprises in some runtimes\n",
    "    return build(\"searchconsole\", \"v1\", credentials=creds, cache_discovery=False)\n",
    "\n",
    "\n",
    "def _iso_date(d: datetime) -> str:\n",
    "    return d.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def _now_iso8601_utc() -> str:\n",
    "    return datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n",
    "\n",
    "\n",
    "def _date_window(window_days: int) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Returns (startDate, endDate) in YYYY-MM-DD.\n",
    "    GSC Search Analytics expects dates in PT in docs, but API accepts date strings; the service handles it.\n",
    "    We use an inclusive window ending \"yesterday\" to avoid partial same-day reporting volatility.\n",
    "    \"\"\"\n",
    "    today_utc = datetime.now(timezone.utc).date()\n",
    "    end = today_utc - timedelta(days=1)\n",
    "    start = end - timedelta(days=window_days - 1)\n",
    "    return start.isoformat(), end.isoformat()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Implement → Query\n",
    "# -----------------------------\n",
    "\n",
    "def query_search_analytics(\n",
    "    service: Any,\n",
    "    *,\n",
    "    site_url: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    dimensions: List[str],\n",
    "    search_type: str = \"web\",\n",
    "    dimension_filter_groups: Optional[List[Dict[str, Any]]] = None,\n",
    "    row_limit: int = 25000,\n",
    "    max_rows: int = 50000,\n",
    "    sleep_between_calls_s: float = 0.2,\n",
    "    max_retries: int = 5,\n",
    ") -> List[SearchRow]:\n",
    "    \"\"\"\n",
    "    Query Search Analytics with pagination via startRow.\n",
    "\n",
    "    Returns the concatenated list of rows.\n",
    "    \"\"\"\n",
    "    rows: List[SearchRow] = []\n",
    "    start_row = 0\n",
    "\n",
    "    while True:\n",
    "        body: Dict[str, Any] = {\n",
    "            \"startDate\": start_date,\n",
    "            \"endDate\": end_date,\n",
    "            \"dimensions\": dimensions,\n",
    "            \"type\": search_type,\n",
    "            \"rowLimit\": min(row_limit, max(1, max_rows - len(rows))),\n",
    "            \"startRow\": start_row,\n",
    "        }\n",
    "        if dimension_filter_groups:\n",
    "            body[\"dimensionFilterGroups\"] = dimension_filter_groups\n",
    "\n",
    "        batch = _execute_with_backoff(\n",
    "            lambda: service.searchanalytics().query(siteUrl=site_url, body=body).execute(),\n",
    "            max_retries=max_retries,\n",
    "        )\n",
    "\n",
    "        batch_rows = batch.get(\"rows\", []) or []\n",
    "        rows.extend(batch_rows)\n",
    "\n",
    "        if sleep_between_calls_s > 0:\n",
    "            time.sleep(sleep_between_calls_s)\n",
    "\n",
    "        # Stop conditions\n",
    "        if not batch_rows:\n",
    "            break\n",
    "        if len(batch_rows) < body[\"rowLimit\"]:\n",
    "            break\n",
    "        if len(rows) >= max_rows:\n",
    "            break\n",
    "\n",
    "        start_row += body[\"rowLimit\"]\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _execute_with_backoff(fn, *, max_retries: int = 5):\n",
    "    \"\"\"\n",
    "    Exponential backoff for transient HTTP errors and quota pressure.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            return fn()\n",
    "        except HttpError as e:\n",
    "            status = getattr(e.resp, \"status\", None)\n",
    "            # Retry 429/5xx + some 403 quota cases\n",
    "            retryable = status in (429, 500, 502, 503, 504, 403)\n",
    "            if not retryable or attempt == max_retries:\n",
    "                raise\n",
    "\n",
    "            # jittered exponential backoff\n",
    "            sleep_s = min(2 ** attempt, 32) + random.random()\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Implement → Filter by searchAppearance\n",
    "# -----------------------------\n",
    "\n",
    "def filter_by_search_appearance(rows: List[SearchRow]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract searchAppearance keys from rows grouped by ['searchAppearance'].\n",
    "    \"\"\"\n",
    "    appearances: List[str] = []\n",
    "    for r in rows:\n",
    "        keys = r.get(\"keys\", [])\n",
    "        if not keys:\n",
    "            continue\n",
    "        # When grouped only by searchAppearance, keys[0] is the appearance label\n",
    "        appearance = str(keys[0]).strip()\n",
    "        if appearance and appearance not in appearances:\n",
    "            appearances.append(appearance)\n",
    "    return appearances\n",
    "\n",
    "\n",
    "def _appearance_filter_group(appearance: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build a filter group that selects a single searchAppearance value.\n",
    "    Using equals avoids weird edge cases with notEquals/notContains.\n",
    "    \"\"\"\n",
    "    return [{\n",
    "        \"groupType\": \"and\",\n",
    "        \"filters\": [{\n",
    "            \"dimension\": \"searchAppearance\",\n",
    "            \"operator\": \"equals\",\n",
    "            \"expression\": appearance,\n",
    "        }],\n",
    "    }]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Normalize → Output shaping\n",
    "# -----------------------------\n",
    "\n",
    "def normalize_gsc_data(\n",
    "    *,\n",
    "    appearance: str,\n",
    "    rows: List[SearchRow],\n",
    "    key_name: str,\n",
    ") -> List[JsonDict]:\n",
    "    \"\"\"\n",
    "    Normalize GSC API rows into stable dicts.\n",
    "\n",
    "    key_name: \"page\" or \"query\"\n",
    "    \"\"\"\n",
    "    normalized: List[JsonDict] = []\n",
    "    for r in rows:\n",
    "        keys = r.get(\"keys\", []) or []\n",
    "        if not keys:\n",
    "            continue\n",
    "\n",
    "        # When filtered by appearance and grouped by [page] or [query], keys[0] is the value.\n",
    "        value = str(keys[0])\n",
    "\n",
    "        clicks = float(r.get(\"clicks\", 0.0) or 0.0)\n",
    "        impressions = float(r.get(\"impressions\", 0.0) or 0.0)\n",
    "        ctr = float(r.get(\"ctr\", (clicks / impressions) if impressions else 0.0) or 0.0)\n",
    "        position = float(r.get(\"position\", 0.0) or 0.0)\n",
    "\n",
    "        normalized.append({\n",
    "            \"searchAppearance\": appearance,\n",
    "            key_name: value,\n",
    "            \"clicks\": clicks,\n",
    "            \"impressions\": impressions,\n",
    "            \"ctr\": ctr,\n",
    "            \"position\": position,\n",
    "        })\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def build_state_patch(\n",
    "    *,\n",
    "    window_days: int,\n",
    "    pages: List[JsonDict],\n",
    "    queries: List[JsonDict],\n",
    ") -> JsonDict:\n",
    "    \"\"\"\n",
    "    Build the required patch structure exactly.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"inputs\": {\n",
    "            \"gsc\": {\n",
    "                \"collected_at\": _now_iso8601_utc(),\n",
    "                \"window_days\": int(window_days),\n",
    "                \"pages\": pages,\n",
    "                \"queries\": queries,\n",
    "                # Not available under the \"Search Analytics only\" constraint:\n",
    "                \"index_coverage\": [],\n",
    "                \"sitemap_status\": [],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Return → Node\n",
    "# -----------------------------\n",
    "\n",
    "def collect_gsc(state: JsonDict, *, auth_mode: str = \"service_account\") -> JsonDict:\n",
    "    \"\"\"\n",
    "    Node: collect_gsc\n",
    "\n",
    "    READS:\n",
    "      - run.domain\n",
    "      - config.integrations.gsc_property_url\n",
    "\n",
    "    WRITES:\n",
    "      - inputs.gsc\n",
    "\n",
    "    Behavior:\n",
    "      1) discovers top searchAppearance segments\n",
    "      2) for each segment, pulls top pages and queries for the same date window\n",
    "      3) normalizes output into a state patch\n",
    "    \"\"\"\n",
    "    domain = _get_path(state, \"run.domain\")\n",
    "    property_url = _get_path(state, \"config.integrations.gsc_property_url\")\n",
    "\n",
    "    if not property_url or not isinstance(property_url, str):\n",
    "        raise ValueError(\"Missing config.integrations.gsc_property_url (expected string).\")\n",
    "\n",
    "    # Optional: allow a property_url template like \"sc-domain:{domain}\"\n",
    "    if isinstance(domain, str) and \"{domain}\" in property_url:\n",
    "        clean_domain = domain.removeprefix(\"sc-domain:\").replace(\"https://\", \"\").replace(\"http://\", \"\").split(\"/\")[0]\n",
    "        property_url = property_url.format(domain=clean_domain)\n",
    "\n",
    "\n",
    "    cfg = GSCQueryConfig(property_url=property_url)\n",
    "\n",
    "    creds = authenticate_gsc(\n",
    "        auth_mode=\"oauth\",\n",
    "        oauth_client_secrets_path=r\"C:\\Users\\Aurum\\vscode\\E-commerce-SEO-Agent\\client_secret_733345724329-vsr9dtaic0aq0e97gbv2eqmegi15tq8t.apps.googleusercontent.com.json\"\n",
    "    )\n",
    "\n",
    "    service = build_gsc_service(creds)\n",
    "\n",
    "    start_date, end_date = _date_window(cfg.window_days)\n",
    "\n",
    "    # 1) Discover appearance segments (top appearances)\n",
    "    appearance_rows = query_search_analytics(\n",
    "        service,\n",
    "        site_url=cfg.property_url,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        dimensions=[\"searchAppearance\"],\n",
    "        search_type=cfg.search_type,\n",
    "        row_limit=min(cfg.row_limit, 5000),  # discovery doesn't need max page size\n",
    "        max_rows=5000,\n",
    "        sleep_between_calls_s=cfg.sleep_between_calls_s,\n",
    "        max_retries=cfg.max_retries,\n",
    "    )\n",
    "    appearances = filter_by_search_appearance(appearance_rows)\n",
    "\n",
    "    # If GSC returns nothing (small/new sites), keep outputs empty but valid.\n",
    "    all_pages: List[JsonDict] = []\n",
    "    all_queries: List[JsonDict] = []\n",
    "\n",
    "    # 2) Pull pages + queries per appearance\n",
    "    for appearance in appearances:\n",
    "        filters = _appearance_filter_group(appearance)\n",
    "\n",
    "        # Pages report (filtered by appearance)\n",
    "        page_rows = query_search_analytics(\n",
    "            service,\n",
    "            site_url=cfg.property_url,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            dimensions=[\"page\"],\n",
    "            search_type=cfg.search_type,\n",
    "            dimension_filter_groups=filters,\n",
    "            row_limit=cfg.row_limit,\n",
    "            max_rows=cfg.max_rows_per_report,\n",
    "            sleep_between_calls_s=cfg.sleep_between_calls_s,\n",
    "            max_retries=cfg.max_retries,\n",
    "        )\n",
    "        all_pages.extend(normalize_gsc_data(appearance=appearance, rows=page_rows, key_name=\"page\"))\n",
    "\n",
    "        # Queries report (filtered by appearance)\n",
    "        query_rows = query_search_analytics(\n",
    "            service,\n",
    "            site_url=cfg.property_url,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            dimensions=[\"query\"],\n",
    "            search_type=cfg.search_type,\n",
    "            dimension_filter_groups=filters,\n",
    "            row_limit=cfg.row_limit,\n",
    "            max_rows=cfg.max_rows_per_report,\n",
    "            sleep_between_calls_s=cfg.sleep_between_calls_s,\n",
    "            max_retries=cfg.max_retries,\n",
    "        )\n",
    "        all_queries.extend(normalize_gsc_data(appearance=appearance, rows=query_rows, key_name=\"query\"))\n",
    "\n",
    "    # 3) Build patch (exact shape)\n",
    "    return build_state_patch(\n",
    "        window_days=cfg.window_days,\n",
    "        pages=all_pages,\n",
    "        queries=all_queries,\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "\n",
    "def _get_path(obj: Dict[str, Any], path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Safe nested dict getter: \"a.b.c\"\n",
    "    \"\"\"\n",
    "    cur: Any = obj\n",
    "    for part in path.split(\".\"):\n",
    "        if not isinstance(cur, dict) or part not in cur:\n",
    "            return None\n",
    "        cur = cur[part]\n",
    "    return cur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f501a396",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://searchconsole.googleapis.com/webmasters/v3/sites/sc-domain%3Anapkin.ai/searchAnalytics/query?alt=json returned \"User does not have sufficient permission for site 'sc-domain:napkin.ai'. See also: https://support.google.com/webmasters/answer/2451999.\". Details: \"[{'message': \"User does not have sufficient permission for site 'sc-domain:napkin.ai'. See also: https://support.google.com/webmasters/answer/2451999.\", 'domain': 'global', 'reason': 'forbidden'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m state \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnapkin.ai\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     }\n\u001b[0;32m      9\u001b[0m }\n\u001b[1;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_gsc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[20], line 377\u001b[0m, in \u001b[0;36mcollect_gsc\u001b[1;34m(state, auth_mode)\u001b[0m\n\u001b[0;32m    374\u001b[0m start_date, end_date \u001b[38;5;241m=\u001b[39m _date_window(cfg\u001b[38;5;241m.\u001b[39mwindow_days)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# 1) Discover appearance segments (top appearances)\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m appearance_rows \u001b[38;5;241m=\u001b[39m \u001b[43mquery_search_analytics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43msite_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperty_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearchAppearance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# discovery doesn't need max page size\u001b[39;49;00m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_between_calls_s\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep_between_calls_s\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m appearances \u001b[38;5;241m=\u001b[39m filter_by_search_appearance(appearance_rows)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# If GSC returns nothing (small/new sites), keep outputs empty but valid.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 190\u001b[0m, in \u001b[0;36mquery_search_analytics\u001b[1;34m(service, site_url, start_date, end_date, dimensions, search_type, dimension_filter_groups, row_limit, max_rows, sleep_between_calls_s, max_retries)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dimension_filter_groups:\n\u001b[0;32m    188\u001b[0m     body[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensionFilterGroups\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dimension_filter_groups\n\u001b[1;32m--> 190\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43m_execute_with_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchanalytics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43msiteUrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msite_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m batch_rows \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, []) \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    196\u001b[0m rows\u001b[38;5;241m.\u001b[39mextend(batch_rows)\n",
      "Cell \u001b[1;32mIn[20], line 220\u001b[0m, in \u001b[0;36m_execute_with_backoff\u001b[1;34m(fn, max_retries)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    222\u001b[0m         status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e\u001b[38;5;241m.\u001b[39mresp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[20], line 191\u001b[0m, in \u001b[0;36mquery_search_analytics.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dimension_filter_groups:\n\u001b[0;32m    188\u001b[0m     body[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensionFilterGroups\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dimension_filter_groups\n\u001b[0;32m    190\u001b[0m batch \u001b[38;5;241m=\u001b[39m _execute_with_backoff(\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchanalytics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43msiteUrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msite_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    192\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[0;32m    193\u001b[0m )\n\u001b[0;32m    195\u001b[0m batch_rows \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, []) \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    196\u001b[0m rows\u001b[38;5;241m.\u001b[39mextend(batch_rows)\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 403 when requesting https://searchconsole.googleapis.com/webmasters/v3/sites/sc-domain%3Anapkin.ai/searchAnalytics/query?alt=json returned \"User does not have sufficient permission for site 'sc-domain:napkin.ai'. See also: https://support.google.com/webmasters/answer/2451999.\". Details: \"[{'message': \"User does not have sufficient permission for site 'sc-domain:napkin.ai'. See also: https://support.google.com/webmasters/answer/2451999.\", 'domain': 'global', 'reason': 'forbidden'}]\">"
     ]
    }
   ],
   "source": [
    "state = {\n",
    "    \"run\": {\"domain\": \"napkin.ai\"},\n",
    "    \"config\": {\n",
    "        \"integrations\": {\n",
    "            # \"gsc_property_url\": \"https://napkin.ai/\"\n",
    "            \"gsc_property_url\": \"sc-domain:napkin.ai\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "result = collect_gsc(state, auth_mode=\"oauth\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"config\"][\"integrations\"][\"gsc_property_url\"] = \"https://napkin.ai/\"\n",
    "\n",
    "\n",
    "result = collect_gsc(state, auth_mode=\"oauth\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6730cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!del gsc_oauth_token.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcb58ad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RefreshError",
     "evalue": "('invalid_scope: Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', {'error': 'invalid_scope', 'error_description': 'Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', 'error_uri': 'https://developers.google.com/identity/protocols/oauth2'})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRefreshError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 13\u001b[0m\n\u001b[0;32m      5\u001b[0m request \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstartDate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-10-01\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendDate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2023-10-31\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimensions\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrowLimit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Authenticate and build service\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m creds \u001b[38;5;241m=\u001b[39m \u001b[43mauthenticate_gsc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moauth_client_secrets_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAurum\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mvscode\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mE-commerce-SEO-Agent\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mclient_secret_733345724329-vsr9dtaic0aq0e97gbv2eqmegi15tq8t.apps.googleusercontent.com.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moauth_token_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgsc_oauth_token.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m service \u001b[38;5;241m=\u001b[39m build_gsc_service(creds)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Execute query using the service already built in Cell 3\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 110\u001b[0m, in \u001b[0;36mauthenticate_gsc\u001b[1;34m(auth_mode, service_account_json_path, oauth_client_secrets_path, oauth_token_path)\u001b[0m\n\u001b[0;32m    107\u001b[0m     creds \u001b[38;5;241m=\u001b[39m UserCredentials\u001b[38;5;241m.\u001b[39mfrom_authorized_user_file(oauth_token_path, scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(SCOPES))\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m creds \u001b[38;5;129;01mand\u001b[39;00m creds\u001b[38;5;241m.\u001b[39mexpired \u001b[38;5;129;01mand\u001b[39;00m creds\u001b[38;5;241m.\u001b[39mrefresh_token:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mcreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m creds \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m creds\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m    112\u001b[0m     flow \u001b[38;5;241m=\u001b[39m InstalledAppFlow\u001b[38;5;241m.\u001b[39mfrom_client_secrets_file(oauth_client_secrets_path, scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(SCOPES))\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\google\\oauth2\\credentials.py:412\u001b[0m, in \u001b[0;36mCredentials.refresh\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refresh_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_secret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    399\u001b[0m ):\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRefreshError(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe credentials do not contain the necessary fields need to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh the access token. You must specify refresh_token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_uri, client_id, and client_secret.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m     )\n\u001b[0;32m    406\u001b[0m (\n\u001b[0;32m    407\u001b[0m     access_token,\n\u001b[0;32m    408\u001b[0m     refresh_token,\n\u001b[0;32m    409\u001b[0m     expiry,\n\u001b[0;32m    410\u001b[0m     grant_response,\n\u001b[0;32m    411\u001b[0m     rapt_token,\n\u001b[1;32m--> 412\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mreauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_grant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_refresh_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_secret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrapt_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rapt_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_reauth_refresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enable_reauth_refresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m access_token\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpiry \u001b[38;5;241m=\u001b[39m expiry\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\google\\oauth2\\reauth.py:370\u001b[0m, in \u001b[0;36mrefresh_grant\u001b[1;34m(request, token_uri, refresh_token, client_id, client_secret, scopes, rapt_token, enable_reauth_refresh)\u001b[0m\n\u001b[0;32m    361\u001b[0m     (\n\u001b[0;32m    362\u001b[0m         response_status_ok,\n\u001b[0;32m    363\u001b[0m         response_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m         request, token_uri, body, headers\u001b[38;5;241m=\u001b[39mmetrics_header\n\u001b[0;32m    367\u001b[0m     )\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response_status_ok:\n\u001b[1;32m--> 370\u001b[0m     \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretryable_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\u001b[38;5;241m.\u001b[39m_handle_refresh_grant_response(response_data, refresh_token) \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m    372\u001b[0m     rapt_token,\n\u001b[0;32m    373\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\google\\oauth2\\_client.py:69\u001b[0m, in \u001b[0;36m_handle_error_response\u001b[1;34m(response_data, retryable_error)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     67\u001b[0m     error_details \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(response_data)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRefreshError(\n\u001b[0;32m     70\u001b[0m     error_details, response_data, retryable\u001b[38;5;241m=\u001b[39mretryable_error\n\u001b[0;32m     71\u001b[0m )\n",
      "\u001b[1;31mRefreshError\u001b[0m: ('invalid_scope: Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', {'error': 'invalid_scope', 'error_description': 'Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', 'error_uri': 'https://developers.google.com/identity/protocols/oauth2'})"
     ]
    }
   ],
   "source": [
    "# Use the OAuth credentials and service already authenticated in the notebook\n",
    "SITE_URL = ['https://www.paddleaurum.com/']\n",
    "\n",
    "# Define query parameters\n",
    "request = {\n",
    "    'startDate': '2023-10-01',\n",
    "    'endDate': '2023-10-31',\n",
    "    'dimensions': ['query', 'page'],\n",
    "    'rowLimit': 100\n",
    "}\n",
    "\n",
    "# Authenticate and build service\n",
    "creds = authenticate_gsc(\n",
    "    auth_mode=\"oauth\",\n",
    "    oauth_client_secrets_path=r\"C:\\Users\\Aurum\\vscode\\E-commerce-SEO-Agent\\client_secret_733345724329-vsr9dtaic0aq0e97gbv2eqmegi15tq8t.apps.googleusercontent.com.json\",\n",
    "    oauth_token_path=\"gsc_oauth_token.json\"\n",
    ")\n",
    "service = build_gsc_service(creds)\n",
    "\n",
    "# Execute query using the service already built in Cell 3\n",
    "response = service.searchanalytics().query(siteUrl=SITE_URL, body=request).execute()\n",
    "\n",
    "# Print results\n",
    "for row in response.get('rows', []):\n",
    "    print(f\"Query: {row['keys'][0]}, Clicks: {row['clicks']}, Impressions: {row['impressions']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f032714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e92ab71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gsc(state: JsonDict, *, auth_mode: str = \"service_account\") -> JsonDict:\n",
    "    \"\"\"\n",
    "    Node: collect_gsc\n",
    "\n",
    "    READS:\n",
    "      - run.domain\n",
    "      - config.integrations.gsc_property_url\n",
    "\n",
    "    WRITES:\n",
    "      - inputs.gsc\n",
    "\n",
    "    Behavior:\n",
    "      1) discovers top searchAppearance segments\n",
    "      2) for each segment, pulls top pages and queries for the same date window\n",
    "      3) normalizes output into a state patch\n",
    "    \"\"\"\n",
    "    domain = _get_path(state, \"run.domain\")\n",
    "    property_url = _get_path(state, \"config.integrations.gsc_property_url\")\n",
    "\n",
    "    if not property_url or not isinstance(property_url, str):\n",
    "        raise ValueError(\"Missing config.integrations.gsc_property_url (expected string).\")\n",
    "\n",
    "    # Optional: allow a property_url template like \"sc-domain:{domain}\"\n",
    "    if isinstance(domain, str) and \"{domain}\" in property_url:\n",
    "        clean_domain = domain.removeprefix(\"sc-domain:\").replace(\"https://\", \"\").replace(\"http://\", \"\").split(\"/\")[0]\n",
    "        property_url = property_url.format(domain=clean_domain)\n",
    "\n",
    "    cfg = GSCQueryConfig(property_url=property_url)\n",
    "\n",
    "    # Get OAuth secrets path from environment or use default\n",
    "    oauth_secrets = os.getenv(\n",
    "        \"GSC_OAUTH_CLIENT_SECRETS\",\n",
    "        r\"C:\\Users\\Aurum\\vscode\\E-commerce-SEO-Agent\\client_secret_733345724329-vsr9dtaic0aq0e97gbv2eqmegi15tq8t.apps.googleusercontent.com.json\"\n",
    "    )\n",
    "\n",
    "    creds = authenticate_gsc(\n",
    "        auth_mode=\"oauth\",\n",
    "        oauth_client_secrets_path=oauth_secrets\n",
    "    )\n",
    "\n",
    "    service = build_gsc_service(creds)\n",
    "    start_date, end_date = _date_window(cfg.window_days)\n",
    "\n",
    "    # 1) Discover appearance segments (top appearances)\n",
    "    appearance_rows = query_search_analytics(\n",
    "        service,\n",
    "        site_url=cfg.property_url,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        dimensions=[\"searchAppearance\"],\n",
    "        search_type=cfg.search_type,\n",
    "        row_limit=min(cfg.row_limit, 5000),\n",
    "        max_rows=5000,\n",
    "        sleep_between_calls_s=cfg.sleep_between_calls_s,\n",
    "        max_retries=cfg.max_retries,\n",
    "    )\n",
    "    appearances = filter_by_search_appearance(appearance_rows)\n",
    "\n",
    "    # If GSC returns nothing (small/new sites), keep outputs empty but valid.\n",
    "    all_pages: List[JsonDict] = []\n",
    "    all_queries: List[JsonDict] = []\n",
    "\n",
    "    # 2) Pull pages + queries per appearance\n",
    "    for appearance in appearances:\n",
    "        filters = _appearance_filter_group(appearance)\n",
    "\n",
    "        # Pages report (filtered by appearance)\n",
    "        page_rows = query_search_analytics(\n",
    "            service,\n",
    "            site_url=cfg.property_url,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            dimensions=[\"page\"],\n",
    "            search_type=cfg.search_type,\n",
    "            dimension_filter_groups=filters,\n",
    "            row_limit=cfg.row_limit,\n",
    "            max_rows=cfg.max_rows_per_report,\n",
    "            sleep_between_calls_s=cfg.sleep_between_calls_s,\n",
    "            max_retries=cfg.max_retries,\n",
    "        )\n",
    "        all_pages.extend(normalize_gsc_data(appearance=appearance, rows=page_rows, key_name=\"page\"))\n",
    "\n",
    "        # Queries report (filtered by appearance)\n",
    "        query_rows = query_search_analytics(\n",
    "            service,\n",
    "            site_url=cfg.property_url,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            dimensions=[\"query\"],\n",
    "            search_type=cfg.search_type,\n",
    "            dimension_filter_groups=filters,\n",
    "            row_limit=cfg.row_limit,\n",
    "            max_rows=cfg.max_rows_per_report,\n",
    "            sleep_between_calls_s=cfg.sleep_between_calls_s,\n",
    "            max_retries=cfg.max_retries,\n",
    "        )\n",
    "        all_queries.extend(normalize_gsc_data(appearance=appearance, rows=query_rows, key_name=\"query\"))\n",
    "\n",
    "    # 3) Build patch (exact shape)\n",
    "    return build_state_patch(\n",
    "        window_days=cfg.window_days,\n",
    "        pages=all_pages,\n",
    "        queries=all_queries,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ce65bc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RefreshError",
     "evalue": "('invalid_scope: Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', {'error': 'invalid_scope', 'error_description': 'Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', 'error_uri': 'https://developers.google.com/identity/protocols/oauth2'})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRefreshError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_gsc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[35], line 22\u001b[0m, in \u001b[0;36mcollect_gsc\u001b[1;34m(state, auth_mode)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Get OAuth secrets path from environment or use default\u001b[39;00m\n\u001b[0;32m     17\u001b[0m oauth_secrets \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGSC_OAUTH_CLIENT_SECRETS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAurum\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvscode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mE-commerce-SEO-Agent\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mclient_secret_733345724329-vsr9dtaic0aq0e97gbv2eqmegi15tq8t.apps.googleusercontent.com.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m creds \u001b[38;5;241m=\u001b[39m \u001b[43mauthenticate_gsc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moauth_client_secrets_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moauth_secrets\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m service \u001b[38;5;241m=\u001b[39m build_gsc_service(creds)\n\u001b[0;32m     28\u001b[0m start_date, end_date \u001b[38;5;241m=\u001b[39m _date_window(cfg\u001b[38;5;241m.\u001b[39mwindow_days)\n",
      "Cell \u001b[1;32mIn[20], line 110\u001b[0m, in \u001b[0;36mauthenticate_gsc\u001b[1;34m(auth_mode, service_account_json_path, oauth_client_secrets_path, oauth_token_path)\u001b[0m\n\u001b[0;32m    107\u001b[0m     creds \u001b[38;5;241m=\u001b[39m UserCredentials\u001b[38;5;241m.\u001b[39mfrom_authorized_user_file(oauth_token_path, scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(SCOPES))\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m creds \u001b[38;5;129;01mand\u001b[39;00m creds\u001b[38;5;241m.\u001b[39mexpired \u001b[38;5;129;01mand\u001b[39;00m creds\u001b[38;5;241m.\u001b[39mrefresh_token:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mcreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m creds \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m creds\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m    112\u001b[0m     flow \u001b[38;5;241m=\u001b[39m InstalledAppFlow\u001b[38;5;241m.\u001b[39mfrom_client_secrets_file(oauth_client_secrets_path, scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(SCOPES))\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\google\\oauth2\\credentials.py:412\u001b[0m, in \u001b[0;36mCredentials.refresh\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refresh_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_secret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    399\u001b[0m ):\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRefreshError(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe credentials do not contain the necessary fields need to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh the access token. You must specify refresh_token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_uri, client_id, and client_secret.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m     )\n\u001b[0;32m    406\u001b[0m (\n\u001b[0;32m    407\u001b[0m     access_token,\n\u001b[0;32m    408\u001b[0m     refresh_token,\n\u001b[0;32m    409\u001b[0m     expiry,\n\u001b[0;32m    410\u001b[0m     grant_response,\n\u001b[0;32m    411\u001b[0m     rapt_token,\n\u001b[1;32m--> 412\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mreauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_grant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_refresh_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_secret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrapt_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rapt_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_reauth_refresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enable_reauth_refresh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m access_token\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpiry \u001b[38;5;241m=\u001b[39m expiry\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\google\\oauth2\\reauth.py:370\u001b[0m, in \u001b[0;36mrefresh_grant\u001b[1;34m(request, token_uri, refresh_token, client_id, client_secret, scopes, rapt_token, enable_reauth_refresh)\u001b[0m\n\u001b[0;32m    361\u001b[0m     (\n\u001b[0;32m    362\u001b[0m         response_status_ok,\n\u001b[0;32m    363\u001b[0m         response_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m         request, token_uri, body, headers\u001b[38;5;241m=\u001b[39mmetrics_header\n\u001b[0;32m    367\u001b[0m     )\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response_status_ok:\n\u001b[1;32m--> 370\u001b[0m     \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_error_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretryable_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\u001b[38;5;241m.\u001b[39m_handle_refresh_grant_response(response_data, refresh_token) \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m    372\u001b[0m     rapt_token,\n\u001b[0;32m    373\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\google\\oauth2\\_client.py:69\u001b[0m, in \u001b[0;36m_handle_error_response\u001b[1;34m(response_data, retryable_error)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     67\u001b[0m     error_details \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(response_data)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRefreshError(\n\u001b[0;32m     70\u001b[0m     error_details, response_data, retryable\u001b[38;5;241m=\u001b[39mretryable_error\n\u001b[0;32m     71\u001b[0m )\n",
      "\u001b[1;31mRefreshError\u001b[0m: ('invalid_scope: Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', {'error': 'invalid_scope', 'error_description': 'Some requested scopes were invalid. {invalid=[a, b, c, e, g, h, i, l, m, ., /, o, p, r, s, t, u, w, :]}', 'error_uri': 'https://developers.google.com/identity/protocols/oauth2'})"
     ]
    }
   ],
   "source": [
    "result = collect_gsc(state, auth_mode=\"oauth\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8fb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea6604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bce3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f6db124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Search Console Data Collection Node\n",
    "Author: Senior Python Engineer with SEO Tooling Expertise\n",
    "Description: Collects GSC Search Analytics data segmented by searchAppearance to infer structured data performance.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "\n",
    "# Google API imports\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "DEFAULT_WINDOW_DAYS = 28\n",
    "MAX_ROWS_PER_QUERY = 25000  # API limit per request [citation:9]\n",
    "SEARCH_TYPES = [\"web\", \"news\", \"video\", \"image\", \"discover\", \"googleNews\"]\n",
    "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "\n",
    "# Mapping from searchAppearance to structured data types [citation:3]\n",
    "SEARCH_APPEARANCE_TO_STRUCTURED_TYPE = {\n",
    "    # Rich Results with GSC reporting\n",
    "    \"AMP Top Story\": \"article\",\n",
    "    \"Education Q&A\": \"education_qa\",\n",
    "    \"FAQ\": \"faq\",\n",
    "    \"Job Listing\": \"job_posting\",\n",
    "    \"Job Details\": \"job_posting\",\n",
    "    \"Merchant Listing\": \"product\",\n",
    "    \"Product Snippet\": \"product\",\n",
    "    \"Q&A\": \"qa_page\",\n",
    "    \"Review Snippet\": \"review\",\n",
    "    \"Recipe Gallery\": \"recipe\",\n",
    "    \"Video\": \"video\",\n",
    "    # Non-rich result appearances\n",
    "    \"AMP non-rich result\": \"amp\",\n",
    "    \"Android App\": \"android_app\",\n",
    "    \"Media Actions\": \"media_actions\",\n",
    "    \"Translated Result\": \"translated\",\n",
    "    \"Web Light\": \"web_light\"\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class PageMetrics:\n",
    "    \"\"\"Normalized page performance metrics\"\"\"\n",
    "    url: str\n",
    "    clicks: int\n",
    "    impressions: int\n",
    "    ctr: float\n",
    "    position: float\n",
    "    search_appearances: List[str]\n",
    "    structured_data_types: List[str]\n",
    "\n",
    "@dataclass\n",
    "class QueryMetrics:\n",
    "    \"\"\"Normalized query performance metrics\"\"\"\n",
    "    query: str\n",
    "    clicks: int\n",
    "    impressions: int\n",
    "    ctr: float\n",
    "    position: float\n",
    "    search_appearances: List[str]\n",
    "    structured_data_types: List[str]\n",
    "\n",
    "@dataclass\n",
    "class SitemapStatus:\n",
    "    \"\"\"Sitemap submission status\"\"\"\n",
    "    path: str\n",
    "    last_submitted: str\n",
    "    is_pending: bool\n",
    "    errors: List[str]\n",
    "\n",
    "class GSCClient:\n",
    "    \"\"\"Google Search Console API client with service account authentication\"\"\"\n",
    "    \n",
    "    def __init__(self, credentials_path: str, property_url: str):\n",
    "        \"\"\"\n",
    "        Initialize GSC client with service account credentials.\n",
    "        \n",
    "        Args:\n",
    "            credentials_path: Path to service account JSON key file\n",
    "            property_url: GSC property URL (e.g., 'https://www.example.com/')\n",
    "        \"\"\"\n",
    "        self.credentials_path = credentials_path\n",
    "        self.property_url = property_url\n",
    "        self.service = None\n",
    "        \n",
    "    def authenticate(self) -> None:\n",
    "        \"\"\"\n",
    "        Authenticate using service account credentials and build GSC service.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If credentials file doesn't exist\n",
    "            ValueError: If authentication fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            credentials = service_account.Credentials.from_service_account_file(\n",
    "                self.credentials_path, scopes=SCOPES\n",
    "            )\n",
    "            self.service = build('searchconsole', 'v1', credentials=credentials)\n",
    "            logger.info(f\"Authenticated to GSC for property: {self.property_url}\")\n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"Credentials file not found: {self.credentials_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Authentication failed: {str(e)}\")\n",
    "            raise ValueError(f\"GSC authentication failed: {str(e)}\")\n",
    "    \n",
    "    def query_search_analytics(\n",
    "        self,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        dimensions: List[str],\n",
    "        row_limit: int = MAX_ROWS_PER_QUERY\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Query Search Analytics API for specified dimensions and date range.\n",
    "        \n",
    "        Args:\n",
    "            start_date: Start date in YYYY-MM-DD format\n",
    "            end_date: End date in YYYY-MM-DD format\n",
    "            dimensions: List of dimensions to group by\n",
    "            row_limit: Maximum rows to return (max 25000)\n",
    "            \n",
    "        Returns:\n",
    "            List of row data from API\n",
    "            \n",
    "        Raises:\n",
    "            HttpError: If API request fails\n",
    "        \"\"\"\n",
    "        if not self.service:\n",
    "            raise RuntimeError(\"GSC service not initialized. Call authenticate() first.\")\n",
    "        \n",
    "        # Validate dimensions include searchAppearance for structured data inference\n",
    "        if \"searchAppearance\" not in dimensions:\n",
    "            logger.warning(\"searchAppearance dimension not included - structured data inference limited\")\n",
    "        \n",
    "        request_body = {\n",
    "            \"startDate\": start_date,\n",
    "            \"endDate\": end_date,\n",
    "            \"dimensions\": dimensions,\n",
    "            \"rowLimit\": min(row_limit, MAX_ROWS_PER_QUERY),\n",
    "            \"dimensionFilterGroups\": [{\n",
    "                \"groupType\": \"and\",\n",
    "                \"filters\": []\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Querying GSC data from {start_date} to {end_date} with dimensions: {dimensions}\")\n",
    "            response = self.service.searchanalytics().query(\n",
    "                siteUrl=self.property_url,\n",
    "                body=request_body\n",
    "            ).execute()\n",
    "            \n",
    "            rows = response.get('rows', [])\n",
    "            logger.info(f\"Retrieved {len(rows)} rows from GSC API\")\n",
    "            return rows\n",
    "            \n",
    "        except HttpError as e:\n",
    "            logger.error(f\"GSC API query failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def fetch_sitemap_status(self) -> List[SitemapStatus]:\n",
    "        \"\"\"\n",
    "        Fetch sitemap submission status for the property.\n",
    "        \n",
    "        Returns:\n",
    "            List of sitemap status objects\n",
    "            \n",
    "        Raises:\n",
    "            HttpError: If API request fails\n",
    "        \"\"\"\n",
    "        if not self.service:\n",
    "            raise RuntimeError(\"GSC service not initialized. Call authenticate() first.\")\n",
    "        \n",
    "        try:\n",
    "            response = self.service.sitemaps().list(siteUrl=self.property_url).execute()\n",
    "            sitemaps = response.get('sitemap', [])\n",
    "            \n",
    "            status_list = []\n",
    "            for sitemap in sitemaps:\n",
    "                status = SitemapStatus(\n",
    "                    path=sitemap.get('path', ''),\n",
    "                    last_submitted=sitemap.get('lastSubmitted', ''),\n",
    "                    is_pending=sitemap.get('isPending', False),\n",
    "                    errors=sitemap.get('errors', [])\n",
    "                )\n",
    "                status_list.append(status)\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(status_list)} sitemaps\")\n",
    "            return status_list\n",
    "            \n",
    "        except HttpError as e:\n",
    "            logger.error(f\"Failed to fetch sitemaps: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "def filter_by_search_appearance(rows: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Separate rows with and without searchAppearance data.\n",
    "    \n",
    "    Args:\n",
    "        rows: Raw rows from GSC API\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (rows_with_appearance, rows_without_appearance)\n",
    "    \"\"\"\n",
    "    with_appearance = []\n",
    "    without_appearance = []\n",
    "    \n",
    "    for row in rows:\n",
    "        keys = row.get('keys', [])\n",
    "        # Check if searchAppearance dimension exists (it's the third dimension in our query)\n",
    "        if len(keys) > 2 and keys[2]:  # searchAppearance is at index 2 when dimensions = ['page', 'query', 'searchAppearance']\n",
    "            with_appearance.append(row)\n",
    "        else:\n",
    "            without_appearance.append(row)\n",
    "    \n",
    "    logger.info(f\"Filtered {len(with_appearance)} rows with searchAppearance, \"\n",
    "                f\"{len(without_appearance)} rows without\")\n",
    "    return with_appearance, without_appearance\n",
    "\n",
    "def normalize_gsc_data(\n",
    "    rows_with_appearance: List[Dict],\n",
    "    rows_without_appearance: List[Dict]\n",
    ") -> Tuple[List[PageMetrics], List[QueryMetrics]]:\n",
    "    \"\"\"\n",
    "    Normalize and aggregate GSC data by page and query.\n",
    "    \n",
    "    Args:\n",
    "        rows_with_appearance: Rows containing searchAppearance data\n",
    "        rows_without_appearance: Rows without searchAppearance data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (normalized_pages, normalized_queries)\n",
    "    \"\"\"\n",
    "    # Aggregate data by page and query\n",
    "    page_aggregates: Dict[str, Dict] = {}\n",
    "    query_aggregates: Dict[str, Dict] = {}\n",
    "    \n",
    "    # Process rows with searchAppearance\n",
    "    for row in rows_with_appearance:\n",
    "        keys = row.get('keys', [])\n",
    "        if len(keys) < 3:\n",
    "            continue\n",
    "            \n",
    "        page, query, appearance = keys[0], keys[1], keys[2]\n",
    "        \n",
    "        # Aggregate page data\n",
    "        if page not in page_aggregates:\n",
    "            page_aggregates[page] = {\n",
    "                'clicks': 0,\n",
    "                'impressions': 0,\n",
    "                'ctr_sum': 0.0,\n",
    "                'position_sum': 0.0,\n",
    "                'count': 0,\n",
    "                'search_appearances': set(),\n",
    "                'structured_data_types': set()\n",
    "            }\n",
    "        \n",
    "        page_data = page_aggregates[page]\n",
    "        page_data['clicks'] += row.get('clicks', 0)\n",
    "        page_data['impressions'] += row.get('impressions', 0)\n",
    "        page_data['ctr_sum'] += row.get('ctr', 0.0)\n",
    "        page_data['position_sum'] += row.get('position', 0.0)\n",
    "        page_data['count'] += 1\n",
    "        page_data['search_appearances'].add(appearance)\n",
    "        \n",
    "        # Map searchAppearance to structured data type\n",
    "        structured_type = SEARCH_APPEARANCE_TO_STRUCTURED_TYPE.get(appearance)\n",
    "        if structured_type:\n",
    "            page_data['structured_data_types'].add(structured_type)\n",
    "        \n",
    "        # Aggregate query data (if query exists)\n",
    "        if query:\n",
    "            if query not in query_aggregates:\n",
    "                query_aggregates[query] = {\n",
    "                    'clicks': 0,\n",
    "                    'impressions': 0,\n",
    "                    'ctr_sum': 0.0,\n",
    "                    'position_sum': 0.0,\n",
    "                    'count': 0,\n",
    "                    'search_appearances': set(),\n",
    "                    'structured_data_types': set()\n",
    "                }\n",
    "            \n",
    "            query_data = query_aggregates[query]\n",
    "            query_data['clicks'] += row.get('clicks', 0)\n",
    "            query_data['impressions'] += row.get('impressions', 0)\n",
    "            query_data['ctr_sum'] += row.get('ctr', 0.0)\n",
    "            query_data['position_sum'] += row.get('position', 0.0)\n",
    "            query_data['count'] += 1\n",
    "            query_data['search_appearances'].add(appearance)\n",
    "            \n",
    "            if structured_type:\n",
    "                query_data['structured_data_types'].add(structured_type)\n",
    "    \n",
    "    # Process rows without searchAppearance (default web search)\n",
    "    for row in rows_without_appearance:\n",
    "        keys = row.get('keys', [])\n",
    "        if not keys:\n",
    "            continue\n",
    "            \n",
    "        page = keys[0] if len(keys) > 0 else ''\n",
    "        query = keys[1] if len(keys) > 1 else ''\n",
    "        \n",
    "        if page:\n",
    "            if page not in page_aggregates:\n",
    "                page_aggregates[page] = {\n",
    "                    'clicks': 0,\n",
    "                    'impressions': 0,\n",
    "                    'ctr_sum': 0.0,\n",
    "                    'position_sum': 0.0,\n",
    "                    'count': 0,\n",
    "                    'search_appearances': set(),\n",
    "                    'structured_data_types': set()\n",
    "                }\n",
    "            \n",
    "            page_data = page_aggregates[page]\n",
    "            page_data['clicks'] += row.get('clicks', 0)\n",
    "            page_data['impressions'] += row.get('impressions', 0)\n",
    "            page_data['ctr_sum'] += row.get('ctr', 0.0)\n",
    "            page_data['position_sum'] += row.get('position', 0.0)\n",
    "            page_data['count'] += 1\n",
    "        \n",
    "        if query:\n",
    "            if query not in query_aggregates:\n",
    "                query_aggregates[query] = {\n",
    "                    'clicks': 0,\n",
    "                    'impressions': 0,\n",
    "                    'ctr_sum': 0.0,\n",
    "                    'position_sum': 0.0,\n",
    "                    'count': 0,\n",
    "                    'search_appearances': set(),\n",
    "                    'structured_data_types': set()\n",
    "                }\n",
    "            \n",
    "            query_data = query_aggregates[query]\n",
    "            query_data['clicks'] += row.get('clicks', 0)\n",
    "            query_data['impressions'] += row.get('impressions', 0)\n",
    "            query_data['ctr_sum'] += row.get('ctr', 0.0)\n",
    "            query_data['position_sum'] += row.get('position', 0.0)\n",
    "            query_data['count'] += 1\n",
    "    \n",
    "    # Convert aggregates to normalized objects\n",
    "    normalized_pages = []\n",
    "    for url, data in page_aggregates.items():\n",
    "        if data['count'] > 0:\n",
    "            page = PageMetrics(\n",
    "                url=url,\n",
    "                clicks=data['clicks'],\n",
    "                impressions=data['impressions'],\n",
    "                ctr=data['ctr_sum'] / data['count'],\n",
    "                position=data['position_sum'] / data['count'],\n",
    "                search_appearances=list(data['search_appearances']),\n",
    "                structured_data_types=list(data['structured_data_types'])\n",
    "            )\n",
    "            normalized_pages.append(page)\n",
    "    \n",
    "    normalized_queries = []\n",
    "    for query_text, data in query_aggregates.items():\n",
    "        if data['count'] > 0 and query_text:\n",
    "            query = QueryMetrics(\n",
    "                query=query_text,\n",
    "                clicks=data['clicks'],\n",
    "                impressions=data['impressions'],\n",
    "                ctr=data['ctr_sum'] / data['count'],\n",
    "                position=data['position_sum'] / data['count'],\n",
    "                search_appearances=list(data['search_appearances']),\n",
    "                structured_data_types=list(data['structured_data_types'])\n",
    "            )\n",
    "            normalized_queries.append(query)\n",
    "    \n",
    "    logger.info(f\"Normalized {len(normalized_pages)} pages and {len(normalized_queries)} queries\")\n",
    "    return normalized_pages, normalized_queries\n",
    "\n",
    "def build_state_patch(\n",
    "    pages: List[PageMetrics],\n",
    "    queries: List[QueryMetrics],\n",
    "    sitemap_statuses: List[SitemapStatus],\n",
    "    window_days: int = DEFAULT_WINDOW_DAYS\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Build the final state patch in the required format.\n",
    "    \n",
    "    Args:\n",
    "        pages: Normalized page metrics\n",
    "        queries: Normalized query metrics\n",
    "        sitemap_statuses: Sitemap status information\n",
    "        window_days: Number of days in the data window\n",
    "        \n",
    "    Returns:\n",
    "        State patch dictionary\n",
    "    \"\"\"\n",
    "    # Convert dataclasses to dictionaries\n",
    "    pages_dict = [asdict(p) for p in pages]\n",
    "    queries_dict = [asdict(q) for q in queries]\n",
    "    sitemap_dict = [asdict(s) for s in sitemap_statuses]\n",
    "    \n",
    "    # Build the state patch\n",
    "    state_patch = {\n",
    "        \"inputs\": {\n",
    "            \"gsc\": {\n",
    "                \"collected_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                \"window_days\": window_days,\n",
    "                \"pages\": pages_dict,\n",
    "                \"queries\": queries_dict,\n",
    "                \"index_coverage\": [],  # Not available via GSC API\n",
    "                \"sitemap_status\": sitemap_dict\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return state_patch\n",
    "\n",
    "def collect_gsc(state: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Main node function to collect GSC data.\n",
    "    \n",
    "    Reads:\n",
    "        - state['run']['domain']\n",
    "        - state['config']['integrations']['gsc_property_url']\n",
    "        - state['config']['integrations']['gsc_credentials_path']\n",
    "    \n",
    "    Writes:\n",
    "        - state['inputs']['gsc']\n",
    "    \n",
    "    Args:\n",
    "        state: Current pipeline state\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with GSC data\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting GSC data collection\")\n",
    "    \n",
    "    # Extract configuration from state\n",
    "    domain = state.get('run', {}).get('domain', '')\n",
    "    property_url = state.get('config', {}).get('integrations', {}).get('gsc_property_url', '')\n",
    "    credentials_path = state.get('config', {}).get('integrations', {}).get('gsc_credentials_path', '')\n",
    "    \n",
    "    # Validate configuration\n",
    "    if not property_url:\n",
    "        property_url = f\"https://{domain}/\" if domain else ''\n",
    "    \n",
    "    if not property_url:\n",
    "        raise ValueError(\"GSC property URL not configured. Set config.integrations.gsc_property_url\")\n",
    "    \n",
    "    if not credentials_path:\n",
    "        raise ValueError(\"GSC credentials path not configured. Set config.integrations.gsc_credentials_path\")\n",
    "    \n",
    "    # Calculate date range (last 28 days) [citation:1]\n",
    "    end_date = datetime.utcnow().date()\n",
    "    start_date = end_date - timedelta(days=DEFAULT_WINDOW_DAYS)\n",
    "    \n",
    "    # Initialize and authenticate GSC client\n",
    "    gsc_client = GSCClient(credentials_path, property_url)\n",
    "    gsc_client.authenticate()\n",
    "    \n",
    "    # Query search analytics data with key dimensions [citation:1][citation:9]\n",
    "    # dimensions = [\"page\", \"query\", \"searchAppearance\"]\n",
    "    \n",
    "    # 1) Core performance data\n",
    "    perf_rows = gsc_client.query_search_analytics(\n",
    "        start_date=start_date.isoformat(),\n",
    "        end_date=end_date.isoformat(),\n",
    "        dimensions=[\"page\", \"query\"],\n",
    "        row_limit=50000\n",
    "    )\n",
    "\n",
    "    # 2) Search appearance data (standalone)\n",
    "    appearance_rows = gsc_client.query_search_analytics(\n",
    "        start_date=start_date.isoformat(),\n",
    "        end_date=end_date.isoformat(),\n",
    "        dimensions=[\"searchAppearance\"],\n",
    "        row_limit=50000\n",
    "    )\n",
    "\n",
    "\n",
    "    rows = gsc_client.query_search_analytics(\n",
    "        start_date=start_date.isoformat(),\n",
    "        end_date=end_date.isoformat(),\n",
    "        dimensions=[\"perf_rows\", \"appearance_rows\"],\n",
    "    )\n",
    "    \n",
    "    # Filter rows by searchAppearance presence\n",
    "    rows_with_appearance, rows_without_appearance = filter_by_search_appearance(rows)\n",
    "    \n",
    "    # Normalize and aggregate data\n",
    "    normalized_pages, normalized_queries = normalize_gsc_data(\n",
    "        rows_with_appearance,\n",
    "        rows_without_appearance\n",
    "    )\n",
    "    \n",
    "    # Fetch sitemap status [citation:2]\n",
    "    sitemap_statuses = gsc_client.fetch_sitemap_status()\n",
    "    \n",
    "    # Build final state patch\n",
    "    state_patch = build_state_patch(\n",
    "        pages=normalized_pages,\n",
    "        queries=normalized_queries,\n",
    "        sitemap_statuses=sitemap_statuses,\n",
    "        window_days=DEFAULT_WINDOW_DAYS\n",
    "    )\n",
    "    \n",
    "    logger.info(\"GSC data collection completed successfully\")\n",
    "    return state_patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c6672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting GSC data collection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during GSC collection: GSC credentials path not configured. Set config.integrations.gsc_credentials_path\n"
     ]
    }
   ],
   "source": [
    "# Example usage and test\n",
    "if __name__ == \"__main__\":\n",
    "    # Example state for testing\n",
    "    example_state = {\n",
    "        \"run\": {\n",
    "            \"domain\": \"paddleaurum.com\"\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"integrations\": {\n",
    "                \"gsc_property_url\": \"https://www.paddleaurum.com/\",\n",
    "                \"gsc_credentials\": str(credentials_path)  # Use the credentials_path variable defined earlier\n",
    "            }\n",
    "        },\n",
    "        \"inputs\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Run the collection\n",
    "        result_state = collect_gsc(example_state)\n",
    "        \n",
    "        # Print sample output\n",
    "        print(\"GSC Data Collection Complete\")\n",
    "        print(f\"Collected at: {result_state['inputs']['gsc']['collected_at']}\")\n",
    "        print(f\"Pages collected: {len(result_state['inputs']['gsc']['pages'])}\")\n",
    "        print(f\"Queries collected: {len(result_state['inputs']['gsc']['queries'])}\")\n",
    "        print(f\"Sitemaps found: {len(result_state['inputs']['gsc']['sitemap_status'])}\")\n",
    "        \n",
    "        # Show first page as example\n",
    "        if result_state['inputs']['gsc']['pages']:\n",
    "            first_page = result_state['inputs']['gsc']['pages'][0]\n",
    "            print(f\"\\nSample page data:\")\n",
    "            print(f\"  URL: {first_page['url']}\")\n",
    "            print(f\"  Clicks: {first_page['clicks']}\")\n",
    "            print(f\"  Search appearances: {first_page['search_appearances']}\")\n",
    "            print(f\"  Inferred structured data: {first_page['structured_data_types']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during GSC collection: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80830da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032060c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8838e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb97750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Search Console Data Collection Node - Enhanced Version\n",
    "Author: Senior Python Engineer with SEO Tooling Expertise\n",
    "Description: Collects GSC Search Analytics data segmented by searchAppearance to infer structured data performance.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Google API imports\n",
    "from google.oauth2 import service_account\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "DEFAULT_WINDOW_DAYS = 28\n",
    "MAX_ROWS_PER_QUERY = 25000\n",
    "SEARCH_TYPES = [\"web\", \"news\", \"video\", \"image\", \"discover\", \"googleNews\"]\n",
    "SCOPES = [\"https://www.googleapis.com/auth/webmasters.readonly\"]\n",
    "\n",
    "# Mapping from searchAppearance to structured data types\n",
    "SEARCH_APPEARANCE_TO_STRUCTURED_TYPE = {\n",
    "    # Rich Results\n",
    "    \"AMP Top Story\": \"article\",\n",
    "    \"Accelerated Mobile Page\": \"amp\",\n",
    "    \"Article\": \"article\",\n",
    "    \"Education Q&A\": \"education_qa\",\n",
    "    \"FAQ\": \"faq\",\n",
    "    \"Fact Check\": \"fact_check\",\n",
    "    \"How-to\": \"how_to\",\n",
    "    \"Job Listing\": \"job_posting\",\n",
    "    \"Job Details\": \"job_posting\",\n",
    "    \"Merchant Listing\": \"product\",\n",
    "    \"Product Snippet\": \"product\",\n",
    "    \"Product Results\": \"product\",\n",
    "    \"Q&A\": \"qa_page\",\n",
    "    \"Recipe\": \"recipe\",\n",
    "    \"Review Snippet\": \"review\",\n",
    "    \"Site Links\": \"site_links\",\n",
    "    \"Video\": \"video\",\n",
    "    \"Web Light\": \"web_light\",\n",
    "    \n",
    "    # SERP Features (Non-Rich)\n",
    "    \"Featured Snippet\": \"featured_snippet\",\n",
    "    \"People Also Ask\": \"people_also_ask\",\n",
    "    \"Top Stories\": \"top_stories\",\n",
    "    \"Twitter Card\": \"twitter_card\",\n",
    "    \n",
    "    # Mobile/App Features\n",
    "    \"Android App\": \"android_app\",\n",
    "    \"App\": \"app\",\n",
    "    \n",
    "    # Default/Organic\n",
    "    \"Organic Result\": \"organic\",\n",
    "    \"Rich Result\": \"rich_result\",\n",
    "    \"Unknown\": \"unknown\"\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class PageMetrics:\n",
    "    \"\"\"Normalized page performance metrics\"\"\"\n",
    "    url: str\n",
    "    clicks: int\n",
    "    impressions: int\n",
    "    ctr: float\n",
    "    position: float\n",
    "    search_appearances: List[str]\n",
    "    structured_data_types: List[str]\n",
    "    last_seen: str  # ISO format date\n",
    "\n",
    "@dataclass\n",
    "class QueryMetrics:\n",
    "    \"\"\"Normalized query performance metrics\"\"\"\n",
    "    query: str\n",
    "    clicks: int\n",
    "    impressions: int\n",
    "    ctr: float\n",
    "    position: float\n",
    "    search_appearances: List[str]\n",
    "    structured_data_types: List[str]\n",
    "    last_seen: str  # ISO format date\n",
    "\n",
    "@dataclass\n",
    "class SitemapStatus:\n",
    "    \"\"\"Sitemap submission status\"\"\"\n",
    "    path: str\n",
    "    last_submitted: str\n",
    "    last_downloaded: str\n",
    "    is_pending: bool\n",
    "    errors: List[str]\n",
    "    warning_count: int\n",
    "    url_count: int\n",
    "\n",
    "@dataclass\n",
    "class IndexCoverage:\n",
    "    \"\"\"Inferred index coverage from performance data\"\"\"\n",
    "    url: str\n",
    "    status: str  # inferred: 'indexed', 'not_indexed', 'unknown'\n",
    "    last_crawled: str  # from last impression date\n",
    "    impressions_last_28d: int\n",
    "    confidence: float  # 0.0 to 1.0 based on data volume\n",
    "\n",
    "class GSCCredentials:\n",
    "    \"\"\"Unified credential handler supporting multiple authentication methods\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_credentials(credential_source: Union[str, Dict]) -> service_account.Credentials:\n",
    "        \"\"\"\n",
    "        Load credentials from multiple sources.\n",
    "        \n",
    "        Args:\n",
    "            credential_source: Can be:\n",
    "                - Path to service account JSON file\n",
    "                - JSON string of service account credentials\n",
    "                - Dictionary of service account credentials\n",
    "                - Path to OAuth 2.0 credentials file\n",
    "                \n",
    "        Returns:\n",
    "            Authenticated credentials object\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If credentials cannot be loaded\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Case 1: Already a credentials dictionary\n",
    "            if isinstance(credential_source, dict):\n",
    "                return service_account.Credentials.from_service_account_info(\n",
    "                    credential_source, scopes=SCOPES\n",
    "                )\n",
    "            \n",
    "            # Case 2: Path to a JSON file\n",
    "            if isinstance(credential_source, str):\n",
    "                # Check if it's a file path\n",
    "                if os.path.exists(credential_source):\n",
    "                    logger.info(f\"Loading credentials from file: {credential_source}\")\n",
    "                    return service_account.Credentials.from_service_account_file(\n",
    "                        credential_source, scopes=SCOPES\n",
    "                    )\n",
    "                \n",
    "                # Case 3: JSON string (common in CI/CD environments)\n",
    "                try:\n",
    "                    creds_dict = json.loads(credential_source)\n",
    "                    required_fields = [\"type\", \"project_id\", \"private_key_id\", \n",
    "                                      \"private_key\", \"client_email\", \"client_id\"]\n",
    "                    \n",
    "                    if all(field in creds_dict for field in required_fields):\n",
    "                        logger.info(\"Loading credentials from JSON string\")\n",
    "                        return service_account.Credentials.from_service_account_info(\n",
    "                            creds_dict, scopes=SCOPES\n",
    "                        )\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "                \n",
    "            raise ValueError(f\"Unsupported credential source: {type(credential_source)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load credentials: {str(e)}\")\n",
    "            raise ValueError(f\"Credential loading failed: {str(e)}\")\n",
    "\n",
    "class GSCClient:\n",
    "    \"\"\"Enhanced Google Search Console API client\"\"\"\n",
    "    \n",
    "    def __init__(self, credentials_source: Union[str, Dict], property_url: str):\n",
    "        \"\"\"\n",
    "        Initialize GSC client with flexible credential source.\n",
    "        \n",
    "        Args:\n",
    "            credentials_source: Service account credentials (file path, JSON string, or dict)\n",
    "            property_url: GSC property URL (e.g., 'https://www.example.com/')\n",
    "        \"\"\"\n",
    "        self.credentials_source = credentials_source\n",
    "        self.property_url = property_url.rstrip('/')\n",
    "        self.service = None\n",
    "        self._validate_property_url()\n",
    "    \n",
    "    def _validate_property_url(self) -> None:\n",
    "        \"\"\"Validate the GSC property URL format.\"\"\"\n",
    "        if not self.property_url:\n",
    "            raise ValueError(\"Property URL cannot be empty\")\n",
    "        \n",
    "        # Accept both domain properties and URL prefix properties\n",
    "        if not (self.property_url.startswith('http://') or \n",
    "                self.property_url.startswith('https://') or\n",
    "                self.property_url.startswith('sc-domain:')):\n",
    "            # Assume it's a domain property\n",
    "            self.property_url = f\"sc-domain:{self.property_url}\"\n",
    "            logger.info(f\"Converted to domain property: {self.property_url}\")\n",
    "    \n",
    "    def authenticate(self) -> None:\n",
    "        \"\"\"\n",
    "        Authenticate using flexible credential source.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If authentication fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            credentials = GSCCredentials.load_credentials(self.credentials_source)\n",
    "            \n",
    "            # Refresh if needed\n",
    "            if credentials.expired and credentials.refresh_token:\n",
    "                credentials.refresh(Request())\n",
    "            \n",
    "            self.service = build('searchconsole', 'v1', credentials=credentials)\n",
    "            \n",
    "            # Test authentication with a simple API call\n",
    "            self._test_authentication()\n",
    "            logger.info(f\"✓ Authenticated to GSC for property: {self.property_url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"✗ Authentication failed: {str(e)}\")\n",
    "            raise ValueError(f\"GSC authentication failed: {str(e)}\")\n",
    "    \n",
    "    def _test_authentication(self) -> None:\n",
    "        \"\"\"Test authentication with a minimal API call.\"\"\"\n",
    "        try:\n",
    "            # Try to list sitemaps (lightweight call)\n",
    "            self.service.sitemaps().list(siteUrl=self.property_url).execute()\n",
    "        except HttpError as e:\n",
    "            # 403 or 401 indicate auth issues\n",
    "            if e.resp.status in [401, 403]:\n",
    "                raise ValueError(f\"Authentication rejected: {e.reason}\")\n",
    "            # Other errors might be due to property access\n",
    "            logger.warning(f\"Auth test returned {e.resp.status}, but continuing\")\n",
    "    \n",
    "    def query_search_analytics(\n",
    "        self,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        dimensions: List[str],\n",
    "        row_limit: int = MAX_ROWS_PER_QUERY,\n",
    "        search_type: str = \"web\"\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Query Search Analytics API with pagination support.\n",
    "        \n",
    "        Args:\n",
    "            start_date: Start date in YYYY-MM-DD format\n",
    "            end_date: End date in YYYY-MM-DD format\n",
    "            dimensions: List of dimensions to group by\n",
    "            row_limit: Maximum rows to return\n",
    "            search_type: Type of search results\n",
    "            \n",
    "        Returns:\n",
    "            List of row data from API\n",
    "        \"\"\"\n",
    "        if not self.service:\n",
    "            raise RuntimeError(\"GSC service not initialized. Call authenticate() first.\")\n",
    "        \n",
    "        all_rows = []\n",
    "        start_row = 0\n",
    "        \n",
    "        while True:\n",
    "            request_body = {\n",
    "                \"startDate\": start_date,\n",
    "                \"endDate\": end_date,\n",
    "                \"dimensions\": dimensions,\n",
    "                \"rowLimit\": min(MAX_ROWS_PER_QUERY, row_limit - len(all_rows)),\n",
    "                \"startRow\": start_row,\n",
    "                \"type\": search_type,\n",
    "                \"dimensionFilterGroups\": [{\n",
    "                    \"groupType\": \"and\",\n",
    "                    \"filters\": []\n",
    "                }]\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                logger.info(f\"Querying rows {start_row} to {start_row + MAX_ROWS_PER_QUERY}\")\n",
    "                response = self.service.searchanalytics().query(\n",
    "                    siteUrl=self.property_url,\n",
    "                    body=request_body\n",
    "                ).execute()\n",
    "                \n",
    "                batch_rows = response.get('rows', [])\n",
    "                if not batch_rows:\n",
    "                    break\n",
    "                    \n",
    "                all_rows.extend(batch_rows)\n",
    "                start_row += len(batch_rows)\n",
    "                \n",
    "                # Check if we have all rows or reached limit\n",
    "                if len(batch_rows) < MAX_ROWS_PER_QUERY or len(all_rows) >= row_limit:\n",
    "                    break\n",
    "                    \n",
    "            except HttpError as e:\n",
    "                logger.error(f\"GSC API query failed: {str(e)}\")\n",
    "                # If it's a 400 error for too many rows, adjust strategy\n",
    "                if \"rowLimit\" in str(e) and start_row > 0:\n",
    "                    logger.warning(\"Reducing batch size due to API limits\")\n",
    "                    break\n",
    "                raise\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(all_rows)} total rows from GSC API\")\n",
    "        return all_rows[:row_limit]\n",
    "    \n",
    "    def fetch_sitemap_status(self) -> List[SitemapStatus]:\n",
    "        \"\"\"Fetch sitemap submission status for the property.\"\"\"\n",
    "        if not self.service:\n",
    "            raise RuntimeError(\"GSC service not initialized. Call authenticate() first.\")\n",
    "        \n",
    "        try:\n",
    "            response = self.service.sitemaps().list(siteUrl=self.property_url).execute()\n",
    "            sitemaps = response.get('sitemap', [])\n",
    "            \n",
    "            status_list = []\n",
    "            for sitemap in sitemaps:\n",
    "                status = SitemapStatus(\n",
    "                    path=sitemap.get('path', ''),\n",
    "                    last_submitted=sitemap.get('lastSubmitted', ''),\n",
    "                    last_downloaded=sitemap.get('lastDownloaded', ''),\n",
    "                    is_pending=sitemap.get('isPending', False),\n",
    "                    errors=sitemap.get('errors', []),\n",
    "                    warning_count=int(sitemap.get('warnings', '0')),\n",
    "                    url_count=int(sitemap.get('contents', [{}])[0].get('submitted', '0') if sitemap.get('contents') else 0)\n",
    "                )\n",
    "                status_list.append(status)\n",
    "            \n",
    "            logger.info(f\"Retrieved {len(status_list)} sitemaps\")\n",
    "            return status_list\n",
    "            \n",
    "        except HttpError as e:\n",
    "            logger.error(f\"Failed to fetch sitemaps: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "def filter_by_search_appearance(rows: List[Dict]) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Separate rows with and without searchAppearance data.\n",
    "    \n",
    "    Args:\n",
    "        rows: Raw rows from GSC API\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (rows_with_appearance, rows_without_appearance)\n",
    "    \"\"\"\n",
    "    with_appearance = []\n",
    "    without_appearance = []\n",
    "    \n",
    "    for row in rows:\n",
    "        keys = row.get('keys', [])\n",
    "        \n",
    "        # Determine if searchAppearance exists based on dimensions\n",
    "        # When dimensions = ['page', 'query', 'searchAppearance'], index 2 is searchAppearance\n",
    "        has_appearance = False\n",
    "        for i, key in enumerate(keys):\n",
    "            # Check if this key matches known search appearance values\n",
    "            if key in SEARCH_APPEARANCE_TO_STRUCTURED_TYPE:\n",
    "                has_appearance = True\n",
    "                break\n",
    "        \n",
    "        if has_appearance:\n",
    "            with_appearance.append(row)\n",
    "        else:\n",
    "            without_appearance.append(row)\n",
    "    \n",
    "    logger.info(f\"Filtered {len(with_appearance)} rows with searchAppearance, \"\n",
    "                f\"{len(without_appearance)} rows without\")\n",
    "    return with_appearance, without_appearance\n",
    "\n",
    "def infer_index_coverage(\n",
    "    pages: List[PageMetrics],\n",
    "    queries: List[QueryMetrics]\n",
    ") -> List[IndexCoverage]:\n",
    "    \"\"\"\n",
    "    Infer index coverage from performance data.\n",
    "    \n",
    "    Since GSC API doesn't provide direct index coverage data,\n",
    "    we infer based on impressions and search appearances.\n",
    "    \n",
    "    Args:\n",
    "        pages: Normalized page metrics\n",
    "        queries: Normalized query metrics\n",
    "        \n",
    "    Returns:\n",
    "        List of inferred index coverage objects\n",
    "    \"\"\"\n",
    "    coverage_list = []\n",
    "    \n",
    "    for page in pages:\n",
    "        # Inference logic:\n",
    "        # 1. High impressions → likely indexed\n",
    "        # 2. Search appearances → confirmed indexed\n",
    "        # 3. No impressions → possibly not indexed\n",
    "        \n",
    "        if page.impressions > 100:\n",
    "            status = \"indexed\"\n",
    "            confidence = min(1.0, page.impressions / 10000)  # Scale confidence with volume\n",
    "        elif page.impressions > 0:\n",
    "            status = \"indexed\"\n",
    "            confidence = 0.5\n",
    "        else:\n",
    "            status = \"unknown\"\n",
    "            confidence = 0.1\n",
    "        \n",
    "        # Boost confidence if we have search appearance data\n",
    "        if page.search_appearances:\n",
    "            confidence = min(1.0, confidence + 0.3)\n",
    "        \n",
    "        coverage = IndexCoverage(\n",
    "            url=page.url,\n",
    "            status=status,\n",
    "            last_crawled=page.last_seen,\n",
    "            impressions_last_28d=page.impressions,\n",
    "            confidence=round(confidence, 2)\n",
    "        )\n",
    "        coverage_list.append(coverage)\n",
    "    \n",
    "    logger.info(f\"Inferred index coverage for {len(coverage_list)} pages\")\n",
    "    return coverage_list\n",
    "\n",
    "def normalize_gsc_data(\n",
    "    rows_with_appearance: List[Dict],\n",
    "    rows_without_appearance: List[Dict],\n",
    "    end_date: datetime\n",
    ") -> Tuple[List[PageMetrics], List[QueryMetrics]]:\n",
    "    \"\"\"\n",
    "    Enhanced normalization with temporal awareness.\n",
    "    \n",
    "    Args:\n",
    "        rows_with_appearance: Rows containing searchAppearance data\n",
    "        rows_without_appearance: Rows without searchAppearance data\n",
    "        end_date: End date of the data window\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (normalized_pages, normalized_queries)\n",
    "    \"\"\"\n",
    "    page_aggregates: Dict[str, Dict] = {}\n",
    "    query_aggregates: Dict[str, Dict] = {}\n",
    "    \n",
    "    def process_row(row: Dict, has_appearance: bool = False):\n",
    "        \"\"\"Helper to process a single row.\"\"\"\n",
    "        keys = row.get('keys', [])\n",
    "        if not keys:\n",
    "            return\n",
    "        \n",
    "        # Extract data with flexible indexing\n",
    "        page = keys[0] if len(keys) > 0 else ''\n",
    "        query = keys[1] if len(keys) > 1 else ''\n",
    "        appearance = keys[2] if len(keys) > 2 and has_appearance else None\n",
    "        \n",
    "        clicks = row.get('clicks', 0)\n",
    "        impressions = row.get('impressions', 0)\n",
    "        ctr = row.get('ctr', 0.0)\n",
    "        position = row.get('position', 0.0)\n",
    "        \n",
    "        # Update page aggregates\n",
    "        if page:\n",
    "            if page not in page_aggregates:\n",
    "                page_aggregates[page] = {\n",
    "                    'clicks': 0,\n",
    "                    'impressions': 0,\n",
    "                    'ctr_weighted_sum': 0.0,\n",
    "                    'position_weighted_sum': 0.0,\n",
    "                    'total_weight': 0,\n",
    "                    'search_appearances': set(),\n",
    "                    'structured_data_types': set(),\n",
    "                    'last_impression_date': None\n",
    "                }\n",
    "            \n",
    "            data = page_aggregates[page]\n",
    "            data['clicks'] += clicks\n",
    "            data['impressions'] += impressions\n",
    "            weight = impressions  # Weight by impressions\n",
    "            data['ctr_weighted_sum'] += ctr * weight\n",
    "            data['position_weighted_sum'] += position * weight\n",
    "            data['total_weight'] += weight\n",
    "            \n",
    "            if appearance:\n",
    "                data['search_appearances'].add(appearance)\n",
    "                structured_type = SEARCH_APPEARANCE_TO_STRUCTURED_TYPE.get(appearance)\n",
    "                if structured_type:\n",
    "                    data['structured_data_types'].add(structured_type)\n",
    "            \n",
    "            # Track most recent date (simplified - using end_date)\n",
    "            data['last_impression_date'] = end_date.isoformat()\n",
    "        \n",
    "        # Update query aggregates\n",
    "        if query:\n",
    "            if query not in query_aggregates:\n",
    "                query_aggregates[query] = {\n",
    "                    'clicks': 0,\n",
    "                    'impressions': 0,\n",
    "                    'ctr_weighted_sum': 0.0,\n",
    "                    'position_weighted_sum': 0.0,\n",
    "                    'total_weight': 0,\n",
    "                    'search_appearances': set(),\n",
    "                    'structured_data_types': set(),\n",
    "                    'last_impression_date': None\n",
    "                }\n",
    "            \n",
    "            data = query_aggregates[query]\n",
    "            data['clicks'] += clicks\n",
    "            data['impressions'] += impressions\n",
    "            weight = impressions\n",
    "            data['ctr_weighted_sum'] += ctr * weight\n",
    "            data['position_weighted_sum'] += position * weight\n",
    "            data['total_weight'] += weight\n",
    "            \n",
    "            if appearance:\n",
    "                data['search_appearances'].add(appearance)\n",
    "                structured_type = SEARCH_APPEARANCE_TO_STRUCTURED_TYPE.get(appearance)\n",
    "                if structured_type:\n",
    "                    data['structured_data_types'].add(structured_type)\n",
    "            \n",
    "            data['last_impression_date'] = end_date.isoformat()\n",
    "    \n",
    "    # Process all rows\n",
    "    for row in rows_with_appearance:\n",
    "        process_row(row, has_appearance=True)\n",
    "    \n",
    "    for row in rows_without_appearance:\n",
    "        process_row(row, has_appearance=False)\n",
    "    \n",
    "    # Convert to normalized objects\n",
    "    normalized_pages = []\n",
    "    for url, data in page_aggregates.items():\n",
    "        if data['total_weight'] > 0:\n",
    "            page = PageMetrics(\n",
    "                url=url,\n",
    "                clicks=data['clicks'],\n",
    "                impressions=data['impressions'],\n",
    "                ctr=data['ctr_weighted_sum'] / data['total_weight'],\n",
    "                position=data['position_weighted_sum'] / data['total_weight'],\n",
    "                search_appearances=list(data['search_appearances']),\n",
    "                structured_data_types=list(data['structured_data_types']),\n",
    "                last_seen=data['last_impression_date'] or end_date.isoformat()\n",
    "            )\n",
    "            normalized_pages.append(page)\n",
    "    \n",
    "    normalized_queries = []\n",
    "    for query_text, data in query_aggregates.items():\n",
    "        if data['total_weight'] > 0 and query_text:\n",
    "            query = QueryMetrics(\n",
    "                query=query_text,\n",
    "                clicks=data['clicks'],\n",
    "                impressions=data['impressions'],\n",
    "                ctr=data['ctr_weighted_sum'] / data['total_weight'],\n",
    "                position=data['position_weighted_sum'] / data['total_weight'],\n",
    "                search_appearances=list(data['search_appearances']),\n",
    "                structured_data_types=list(data['structured_data_types']),\n",
    "                last_seen=data['last_impression_date'] or end_date.isoformat()\n",
    "            )\n",
    "            normalized_queries.append(query)\n",
    "    \n",
    "    logger.info(f\"Normalized {len(normalized_pages)} pages and {len(normalized_queries)} queries\")\n",
    "    return normalized_pages, normalized_queries\n",
    "\n",
    "def build_state_patch(\n",
    "    pages: List[PageMetrics],\n",
    "    queries: List[QueryMetrics],\n",
    "    sitemap_statuses: List[SitemapStatus],\n",
    "    window_days: int = DEFAULT_WINDOW_DAYS\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Build the final state patch in the required format.\n",
    "    \n",
    "    Args:\n",
    "        pages: Normalized page metrics\n",
    "        queries: Normalized query metrics\n",
    "        sitemap_statuses: Sitemap status information\n",
    "        window_days: Number of days in the data window\n",
    "        \n",
    "    Returns:\n",
    "        State patch dictionary\n",
    "    \"\"\"\n",
    "    # Infer index coverage\n",
    "    index_coverage = infer_index_coverage(pages, queries)\n",
    "    \n",
    "    # Convert dataclasses to dictionaries\n",
    "    pages_dict = [asdict(p) for p in pages]\n",
    "    queries_dict = [asdict(q) for q in queries]\n",
    "    sitemap_dict = [asdict(s) for s in sitemap_statuses]\n",
    "    coverage_dict = [asdict(c) for c in index_coverage]\n",
    "    \n",
    "    # Build the state patch\n",
    "    state_patch = {\n",
    "        \"inputs\": {\n",
    "            \"gsc\": {\n",
    "                \"collected_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                \"window_days\": window_days,\n",
    "                \"pages\": pages_dict,\n",
    "                \"queries\": queries_dict,\n",
    "                \"index_coverage\": coverage_dict,\n",
    "                \"sitemap_status\": sitemap_dict\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return state_patch\n",
    "\n",
    "def collect_gsc(state: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Main node function to collect GSC data with enhanced error handling.\n",
    "    \n",
    "    Reads:\n",
    "        - state['run']['domain']\n",
    "        - state['config']['integrations']['gsc_property_url']\n",
    "        - state['config']['integrations']['gsc_credentials']\n",
    "        \n",
    "    Writes:\n",
    "        - state['inputs']['gsc']\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting GSC data collection\")\n",
    "    \n",
    "    try:\n",
    "        # Extract configuration from state\n",
    "        domain = state.get('run', {}).get('domain', '')\n",
    "        integrations = state.get('config', {}).get('integrations', {})\n",
    "        \n",
    "        property_url = integrations.get('gsc_property_url')\n",
    "        credentials_source = integrations.get('gsc_credentials')\n",
    "        \n",
    "        # Fallback options for credentials\n",
    "        if not credentials_source:\n",
    "            credentials_source = integrations.get('gsc_service_account')\n",
    "        if not credentials_source:\n",
    "            credentials_source = integrations.get('gsc_credentials_path')\n",
    "        \n",
    "        # Validate configuration\n",
    "        if not property_url and domain:\n",
    "            property_url = f\"https://{domain}/\"\n",
    "        \n",
    "        if not property_url:\n",
    "            raise ValueError(\n",
    "                \"GSC property URL not configured. \"\n",
    "                \"Set config.integrations.gsc_property_url or provide domain in run.domain\"\n",
    "            )\n",
    "        \n",
    "        if not credentials_source:\n",
    "            # Check for environment variable as last resort\n",
    "            env_creds = os.getenv('GSC_CREDENTIALS_JSON')\n",
    "            if env_creds:\n",
    "                credentials_source = env_creds\n",
    "                logger.info(\"Using credentials from GSC_CREDENTIALS_JSON environment variable\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"GSC credentials not found. Provide one of: \"\n",
    "                    \"config.integrations.gsc_credentials, \"\n",
    "                    \"config.integrations.gsc_service_account, \"\n",
    "                    \"config.integrations.gsc_credentials_path, \"\n",
    "                    \"or GSC_CREDENTIALS_JSON environment variable\"\n",
    "                )\n",
    "        \n",
    "        # Calculate date range\n",
    "        # end_date = datetime.utcnow().date()\n",
    "        # start_date = end_date - timedelta(days=DEFAULT_WINDOW_DAYS)\n",
    "\n",
    "        # Calculate date range\n",
    "        end_date = datetime.utcnow().date()\n",
    "        start_date = end_date - timedelta(days=DEFAULT_WINDOW_DAYS)\n",
    "\n",
    "        # 🔥 Add this line to grab search_type safely\n",
    "        search_type = integrations.get(\"gsc_search_type\", \"web\")\n",
    "\n",
    "        \n",
    "        # Initialize and authenticate GSC client\n",
    "        logger.info(f\"Initializing GSC client for property: {property_url}\")\n",
    "        gsc_client = GSCClient(credentials_source, property_url)\n",
    "        gsc_client.authenticate()\n",
    "        \n",
    "        # Query search analytics data\n",
    "        # dimensions = [\"page\", \"query\", \"searchAppearance\"]\n",
    "        # dimensions = [\"searchAppearance\"]\n",
    "\n",
    "        # --- Query 1: Just searchAppearance ---\n",
    "        rows_with_appearance = gsc_client.query_search_analytics(\n",
    "            start_date=start_date.isoformat(),\n",
    "            end_date=end_date.isoformat(),\n",
    "            dimensions=[\"searchAppearance\"],\n",
    "            row_limit=50000,\n",
    "            search_type=search_type\n",
    "        )\n",
    "\n",
    "        # --- Query 2: Just page + query ---\n",
    "        rows_without_appearance = gsc_client.query_search_analytics(\n",
    "            start_date=start_date.isoformat(),\n",
    "            end_date=end_date.isoformat(),\n",
    "            dimensions=[\"page\", \"query\"],\n",
    "            row_limit=50000,\n",
    "            search_type=search_type\n",
    "        )\n",
    "\n",
    "\n",
    "        logger.info(f\"Querying data from {start_date} to {end_date}\")\n",
    "        \n",
    "        rows = gsc_client.query_search_analytics(\n",
    "            start_date=start_date.isoformat(),\n",
    "            end_date=end_date.isoformat(),\n",
    "            dimensions=[rows_with_appearance, rows_without_appearance],\n",
    "            row_limit=50000  # Increased for better coverage\n",
    "        )\n",
    "        \n",
    "        if not rows:\n",
    "            logger.warning(\"No data returned from GSC API\")\n",
    "            # Return empty but valid structure\n",
    "            return build_state_patch([], [], [], DEFAULT_WINDOW_DAYS)\n",
    "        \n",
    "        # Filter and normalize data\n",
    "        rows_with_appearance, rows_without_appearance = filter_by_search_appearance(rows)\n",
    "        \n",
    "        normalized_pages, normalized_queries = normalize_gsc_data(\n",
    "            rows_with_appearance,\n",
    "            rows_without_appearance,\n",
    "            end_date\n",
    "        )\n",
    "        \n",
    "        # Fetch additional data\n",
    "        sitemap_statuses = gsc_client.fetch_sitemap_status()\n",
    "        \n",
    "        # Build final state patch\n",
    "        state_patch = build_state_patch(\n",
    "            pages=normalized_pages,\n",
    "            queries=normalized_queries,\n",
    "            sitemap_statuses=sitemap_statuses,\n",
    "            window_days=DEFAULT_WINDOW_DAYS\n",
    "        )\n",
    "        \n",
    "        logger.info(\"✓ GSC data collection completed successfully\")\n",
    "        logger.info(f\"  Pages: {len(normalized_pages)}\")\n",
    "        logger.info(f\"  Queries: {len(normalized_queries)}\")\n",
    "        logger.info(f\"  Sitemaps: {len(sitemap_statuses)}\")\n",
    "        \n",
    "        return state_patch\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ GSC collection failed: {str(e)}\", exc_info=True)\n",
    "        \n",
    "        # Return minimal valid structure even on error\n",
    "        error_patch = {\n",
    "            \"inputs\": {\n",
    "                \"gsc\": {\n",
    "                    \"collected_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "                    \"window_days\": DEFAULT_WINDOW_DAYS,\n",
    "                    \"pages\": [],\n",
    "                    \"queries\": [],\n",
    "                    \"index_coverage\": [],\n",
    "                    \"sitemap_status\": [],\n",
    "                    \"error\": str(e),\n",
    "                    \"success\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return error_patch\n",
    "\n",
    "# Example usage with better credential handling\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Test with multiple credential formats\n",
    "#     test_cases = [\n",
    "#         # {\n",
    "#         #     \"name\": \"File Path Example\",\n",
    "#         #     \"state\": {\n",
    "#         #         \"run\": {\"domain\": \"example.com\"},\n",
    "#         #         \"config\": {\n",
    "#         #             \"integrations\": {\n",
    "#         #                 \"gsc_property_url\": \"https://www.paddleaurum.com/\",\n",
    "#         #                 \"gsc_credentials_path\": \"secret\\seo-agent-486408-51492a0c6774.json\"\n",
    "#         #             }\n",
    "#         #         }\n",
    "#         #     }\n",
    "#         # },\n",
    "#         {\n",
    "#             \"name\": \"JSON String Example\",\n",
    "#             \"state\": {\n",
    "#                 \"run\": {\"domain\": \"paddleaurum.com\"},\n",
    "#                 \"config\": {\n",
    "#                     \"integrations\": {\n",
    "#                         \"gsc_property_url\": \"sc-domain:paddleaurum.com\",\n",
    "#                         \"gsc_credentials\": \"\"\"\n",
    "#                         {\"type\": \"service_account\",\n",
    "#                             \"project_id\": \"seo-agent-486408\",\n",
    "#                             \"private_key_id\": \"51492a0c6774672a3e75acf377280e31d9a41713\",\n",
    "#                             \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDyHmPb2Fea4ScV\\nMfbqeQHSfFV+A8reH6V/r8S+bDiRKzZPaIdGrkFrd/pj7oaTnpEgqQeMB4GSsyc3\\nIh6dNDOjly2LfUdcxG3k66fze5SsiQlNiw6qJb29fQG4OXaJYPwLfyOuUzLs25gq\\n6sZEkJ3Far48cswy0x+MB2AwqXaR7dQ7F1JpSAfdIoF+cO3eO+G2pxI2Ehg0SmNb\\nw+y3ICeapiojT7sLrRYYsy/4MJxctVyn3RGA4lE3k3ZXperfsw3jmRkQ7laf7nwZ\\nn1xkhMuf3dBrBQrKfEzsEq5kF3ePoPJALscbSQIvMMStpCiEtK5SlcsOJ4qOItw4\\neLqCxtCxAgMBAAECggEAHui3SdUcFfchZKjMw1EwZN0fkudMCkBTAfJ/9Ole7VVt\\naQTIpELRsjUOX5yPTKAlGdzKTFuW9JHvcy/lZZ9I6lz94P1c02B2QQ++kKxZpg8W\\ng3I5rIoF539ibVDUq6QYeSVBels/uJIuImh3aEBoC9BdOd1T6Tcfo2I7quqvauCI\\nsmfvpAP3LVZ0ODlduUBovVavISA48yOQfmed2uQCuMihj35NrJ/qZItMQEkELZfa\\nWfJFiyBM9fMcVfFS3tQh1sQsoNKnSDovrFGh4wtOMSqPlseeAcnjZasX/0Q1rjo6\\nGZ3COaETJ83g+6wXNDciHa3AHYhd1sJ5OL8AMOGMWQKBgQD7Gtn5d09D9Uiv6q8U\\nH8MEj6TanRcXLR0Lb1MRmjilI2lJNmSs0AofuxCqcMW6QBzo2eeDeeU7O5a5TliI\\njRVRv7xdq/TSBmA3jKlq5Yue57J77VrcIS/Vn1q4Wdro/Ia82Bc6nCtqIeXf6sGN\\nSsALepYHCeNUkCUElBp5uUJOzQKBgQD21rFYB1tNCaCVO2b5ty+JF2vT8esynd93\\niFHxmjVJb94+Ox24myVq1tAyGbMM3o6Cd5t1agZHKc5P6+R72i51mVf8IANSTM0S\\n2+UP57xafsZJcyXh0RhKVXhPzdwkwKp8CDd/iqhBoYZKj1sDnj8KdH9tgcATkLc8\\nzvBUtdIBdQKBgQDXI3xaJoS9PuxV/IaggU3/HGsr+qeL7dUStQDA9hdONXbBiV7c\\nSSpDYWy3+wMNvlyGjBu1I7zo8PcEMUHdTLNVZScoQmnxgBDzxwpoUd87+FuNniDY\\ntX5cUrnRdPr/30w5hBLy3la8Cer/3AyU19SOgSsFQM2K6C0vLQ05+SX8iQKBgDLL\\nLyc8LE+IAaFz3dbZGVEOnsO4bnu1/Pwt8x7SatC30nXf2FfapTJ9Dc6hdzTYv16d\\nmkHpFW2jbq8HjbbmyNSuP2rEc6qF31VYJqTFPP8qQ0duCIeVbXgualmRtRFjDDIF\\nxtXaBJGs2WOSaFowQCy4mFhCcPIVv38l3lKwL4zZAoGBAJR3N4Y/6uyI57vUp6mI\\nWsYZASmLwNyNdkgYOaJYPrtuhqNEosb7MmkLsIbsm2iw7g7W9H5t7y49mwRg3Lc1\\nZpvx3CMWvXHvMgxS3vMfbcA82gCQvz0Cb7J9i7I1Kbqw9yg4LjFXmKhWNb/sUKEW\\nIU9J1MJKHT38qZwpY56zUrD4\\n-----END PRIVATE KEY-----\\n\",\n",
    "#                             \"client_email\": \"gsc-data-collection@seo-agent-486408.iam.gserviceaccount.com\",\n",
    "#                             \"client_id\": \"111419250013032075846\",\n",
    "#                             \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "#                             \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "#                             \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "#                             \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gsc-data-collection%40seo-agent-486408.iam.gserviceaccount.com\",\n",
    "#                             \"universe_domain\": \"googleapis.com\"\n",
    "#                         }    \n",
    "\n",
    "#                         \"\"\"\n",
    "#                     }\n",
    "#                 }\n",
    "#             }\n",
    "#         },\n",
    "#         # {\n",
    "#     #         \"name\": \"Minimal Domain Example\",\n",
    "#     #         \"state\": {\n",
    "#     #             \"run\": {\"domain\": \"example.com\"},\n",
    "#     #             \"config\": {\n",
    "#     #                 \"integrations\": {\n",
    "#     #                     \"gsc_property_url\": \"example.com\",\n",
    "#     #                     \"gsc_credentials\": {\n",
    "#     #                         \"type\": \"service_account\",\n",
    "#     #                         \"project_id\": \"test\",\n",
    "#     #                         \"private_key_id\": \"test\",\n",
    "#     #                         \"private_key\": \"test\",\n",
    "#     #                         \"client_email\": \"test@example.com\",\n",
    "#     #                         \"client_id\": \"test\"\n",
    "#     #                     }\n",
    "#     #                 }\n",
    "#     #             }\n",
    "#     #         }\n",
    "#     #     }\n",
    "#     ]\n",
    "    \n",
    "#     print(\"GSC Data Collection Node - Enhanced Version\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     for test_case in test_cases:\n",
    "#         print(f\"\\nTesting: {test_case['name']}\")\n",
    "#         print(\"-\" * 30)\n",
    "        \n",
    "#         try:\n",
    "#             # Set environment variable for testing\n",
    "#             if \"JSON String Example\" in test_case[\"name\"]:\n",
    "#                 os.environ['GSC_CREDENTIALS_JSON'] = test_case['state']['config']['integrations']['gsc_credentials']\n",
    "            \n",
    "#             result = collect_gsc(test_case['state'])\n",
    "            \n",
    "#             if result['inputs']['gsc'].get('success', True):\n",
    "#                 print(f\"✓ Collection successful\")\n",
    "#                 print(f\"  Pages collected: {len(result['inputs']['gsc']['pages'])}\")\n",
    "#                 print(f\"  Queries collected: {len(result['inputs']['gsc']['queries'])}\")\n",
    "#                 print(f\"  Sitemaps: {len(result['inputs']['gsc']['sitemap_status'])}\")\n",
    "#                 print(f\"  Index coverage inferred: {len(result['inputs']['gsc']['index_coverage'])}\")\n",
    "#             else:\n",
    "#                 print(f\"✗ Collection failed: {result['inputs']['gsc'].get('error')}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"✗ Test failed: {str(e)}\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 50)\n",
    "#     print(\"Configuration Notes:\")\n",
    "#     print(\"1. Credentials can be: file path, JSON string, or Python dict\")\n",
    "#     print(\"2. Property URL can be: https://domain.com/, sc-domain:domain.com, or just domain.com\")\n",
    "#     print(\"3. Index coverage is inferred from performance data\")\n",
    "#     print(\"4. Structured data types are mapped from searchAppearance values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df20f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytest -Uq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8651a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from datetime import datetime\n",
    "from unittest.mock import MagicMock, patch\n",
    "\n",
    "# import your node\n",
    "# from your_module.collect_gsc import collect_gsc\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_gsc_service():\n",
    "    \"\"\"\n",
    "    Builds a fake googleapiclient service object:\n",
    "    service.searchanalytics().query().execute()\n",
    "    \"\"\"\n",
    "    service = MagicMock()\n",
    "\n",
    "    # Fake Search Analytics rows\n",
    "    service.searchanalytics.return_value.query.return_value.execute.return_value = {\n",
    "        \"rows\": [\n",
    "            {\n",
    "                \"keys\": [\"https://paddleaurum.com/page-1\", \"Rich Result\"],\n",
    "                \"clicks\": 10,\n",
    "                \"impressions\": 100,\n",
    "                \"ctr\": 0.1,\n",
    "                \"position\": 3.2,\n",
    "            },\n",
    "            {\n",
    "                \"keys\": [\"https://paddleaurum.com/page-2\", \"FAQ\"],\n",
    "                \"clicks\": 5,\n",
    "                \"impressions\": 40,\n",
    "                \"ctr\": 0.125,\n",
    "                \"position\": 6.0,\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return service\n",
    "\n",
    "\n",
    "@patch(\"your_module.collect_gsc.authenticate_gsc\")\n",
    "def test_collect_gsc_basic(mock_authenticate, mock_gsc_service):\n",
    "    \"\"\"\n",
    "    Basic happy-path test:\n",
    "    - valid credentials\n",
    "    - valid search analytics rows\n",
    "    - correct output structure\n",
    "    \"\"\"\n",
    "\n",
    "    mock_authenticate.return_value = mock_gsc_service\n",
    "\n",
    "    state = {\n",
    "        \"run\": {\n",
    "            \"domain\": \"example.com\"\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"integrations\": {\n",
    "                \"gsc_property_url\": \"https://paddleaurum.com/\",\n",
    "                \"gsc_credentials\": {\n",
    "                    \"type\": \"service_account\",\n",
    "                    \"project_id\": \"seo-agent-486408\",\n",
    "                    \"private_key_id\": \"51492a0c6774672a3e75acf377280e31d9a41713\",\n",
    "                    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDyHmPb2Fea4ScV\\nMfbqeQHSfFV+A8reH6V/r8S+bDiRKzZPaIdGrkFrd/pj7oaTnpEgqQeMB4GSsyc3\\nIh6dNDOjly2LfUdcxG3k66fze5SsiQlNiw6qJb29fQG4OXaJYPwLfyOuUzLs25gq\\n6sZEkJ3Far48cswy0x+MB2AwqXaR7dQ7F1JpSAfdIoF+cO3eO+G2pxI2Ehg0SmNb\\nw+y3ICeapiojT7sLrRYYsy/4MJxctVyn3RGA4lE3k3ZXperfsw3jmRkQ7laf7nwZ\\nn1xkhMuf3dBrBQrKfEzsEq5kF3ePoPJALscbSQIvMMStpCiEtK5SlcsOJ4qOItw4\\neLqCxtCxAgMBAAECggEAHui3SdUcFfchZKjMw1EwZN0fkudMCkBTAfJ/9Ole7VVt\\naQTIpELRsjUOX5yPTKAlGdzKTFuW9JHvcy/lZZ9I6lz94P1c02B2QQ++kKxZpg8W\\ng3I5rIoF539ibVDUq6QYeSVBels/uJIuImh3aEBoC9BdOd1T6Tcfo2I7quqvauCI\\nsmfvpAP3LVZ0ODlduUBovVavISA48yOQfmed2uQCuMihj35NrJ/qZItMQEkELZfa\\nWfJFiyBM9fMcVfFS3tQh1sQsoNKnSDovrFGh4wtOMSqPlseeAcnjZasX/0Q1rjo6\\nGZ3COaETJ83g+6wXNDciHa3AHYhd1sJ5OL8AMOGMWQKBgQD7Gtn5d09D9Uiv6q8U\\nH8MEj6TanRcXLR0Lb1MRmjilI2lJNmSs0AofuxCqcMW6QBzo2eeDeeU7O5a5TliI\\njRVRv7xdq/TSBmA3jKlq5Yue57J77VrcIS/Vn1q4Wdro/Ia82Bc6nCtqIeXf6sGN\\nSsALepYHCeNUkCUElBp5uUJOzQKBgQD21rFYB1tNCaCVO2b5ty+JF2vT8esynd93\\niFHxmjVJb94+Ox24myVq1tAyGbMM3o6Cd5t1agZHKc5P6+R72i51mVf8IANSTM0S\\n2+UP57xafsZJcyXh0RhKVXhPzdwkwKp8CDd/iqhBoYZKj1sDnj8KdH9tgcATkLc8\\nzvBUtdIBdQKBgQDXI3xaJoS9PuxV/IaggU3/HGsr+qeL7dUStQDA9hdONXbBiV7c\\nSSpDYWy3+wMNvlyGjBu1I7zo8PcEMUHdTLNVZScoQmnxgBDzxwpoUd87+FuNniDY\\ntX5cUrnRdPr/30w5hBLy3la8Cer/3AyU19SOgSsFQM2K6C0vLQ05+SX8iQKBgDLL\\nLyc8LE+IAaFz3dbZGVEOnsO4bnu1/Pwt8x7SatC30nXf2FfapTJ9Dc6hdzTYv16d\\nmkHpFW2jbq8HjbbmyNSuP2rEc6qF31VYJqTFPP8qQ0duCIeVbXgualmRtRFjDDIF\\nxtXaBJGs2WOSaFowQCy4mFhCcPIVv38l3lKwL4zZAoGBAJR3N4Y/6uyI57vUp6mI\\nWsYZASmLwNyNdkgYOaJYPrtuhqNEosb7MmkLsIbsm2iw7g7W9H5t7y49mwRg3Lc1\\nZpvx3CMWvXHvMgxS3vMfbcA82gCQvz0Cb7J9i7I1Kbqw9yg4LjFXmKhWNb/sUKEW\\nIU9J1MJKHT38qZwpY56zUrD4\\n-----END PRIVATE KEY-----\\n\",\n",
    "                    \"client_email\": \"gsc-data-collection@seo-agent-486408.iam.gserviceaccount.com\",\n",
    "                    \"client_id\": \"111419250013032075846\",\n",
    "                    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "                    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "                    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "                    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gsc-data-collection%40seo-agent-486408.iam.gserviceaccount.com\",\n",
    "                    \"universe_domain\": \"googleapis.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = collect_gsc(state)\n",
    "\n",
    "    assert \"inputs\" in result\n",
    "    assert \"gsc\" in result[\"inputs\"]\n",
    "\n",
    "    gsc = result[\"inputs\"][\"gsc\"]\n",
    "\n",
    "    # ---- schema assertions ----\n",
    "    assert set(gsc.keys()) == {\n",
    "        \"collected_at\",\n",
    "        \"window_days\",\n",
    "        \"pages\",\n",
    "        \"queries\",\n",
    "        \"index_coverage\",\n",
    "        \"sitemap_status\",\n",
    "    }\n",
    "\n",
    "    assert isinstance(gsc[\"pages\"], list)\n",
    "    assert isinstance(gsc[\"queries\"], list)\n",
    "    assert isinstance(gsc[\"index_coverage\"], list)\n",
    "    assert isinstance(gsc[\"sitemap_status\"], list)\n",
    "\n",
    "    # ---- semantic assertions ----\n",
    "    assert len(gsc[\"pages\"]) == 2\n",
    "\n",
    "    page = gsc[\"pages\"][0]\n",
    "    assert \"url\" in page\n",
    "    assert \"search_appearances\" in page\n",
    "    assert \"structured_data_types\" in page\n",
    "\n",
    "    assert page[\"structured_data_types\"]  # inferred from searchAppearance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2122282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collect_gsc_queries_present(mock_authenticate, mock_gsc_service):\n",
    "    mock_authenticate.return_value = mock_gsc_service\n",
    "\n",
    "    state = {\n",
    "        \"run\": {\"domain\": \"example.com\"},\n",
    "        \"config\": {\"integrations\": {\"gsc_credentials\": \"dummy\"}}\n",
    "    }\n",
    "\n",
    "    result = collect_gsc(state)\n",
    "    queries = result[\"inputs\"][\"gsc\"][\"queries\"]\n",
    "\n",
    "    assert isinstance(queries, list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "36fda7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GSC_CREDENTIALS_JSON\"] = open(\n",
    "    r\"C:\\Users\\Aurum\\vscode\\E-commerce-SEO-Agent\\secret\\seo-agent-486408-51492a0c6774.json\", \"r\"\n",
    ").read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48fe49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_collect_gsc_node():\n",
    "    state = {\n",
    "        \"run\": {\n",
    "            \"domain\": \"paddleaurum.com\"\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"integrations\": {\n",
    "                \"gsc_property_url\": \"sc-domain:paddleaurum.com\",\n",
    "                \"gsc_window_days\": 28,\n",
    "                \"gsc_search_type\": \"web\",\n",
    "                \"gsc_row_limit\": 50_000,\n",
    "                \"gsc_max_pages\": 10_000,\n",
    "                \"gsc_max_queries\": 10_000,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    patch = collect_gsc(state)\n",
    "    gsc = patch[\"inputs\"][\"gsc\"]\n",
    "\n",
    "    print(\"✅ GSC collection complete\")\n",
    "    print(\"Pages:\", len(gsc[\"pages\"]))\n",
    "    print(\"Queries:\", len(gsc[\"queries\"]))\n",
    "    print(\"Index coverage:\", len(gsc[\"index_coverage\"]))\n",
    "    print(\"Sitemaps:\", len(gsc[\"sitemap_status\"]))\n",
    "\n",
    "    return patch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f6091f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting GSC data collection\n",
      "INFO:__main__:Using credentials from GSC_CREDENTIALS_JSON environment variable\n",
      "INFO:__main__:Initializing GSC client for property: sc-domain:paddleaurum.com\n",
      "INFO:__main__:Loading credentials from JSON string\n",
      "INFO:googleapiclient.discovery_cache:file_cache is only supported with oauth2client<4.0.0\n",
      "INFO:__main__:✓ Authenticated to GSC for property: sc-domain:paddleaurum.com\n",
      "INFO:__main__:Querying rows 0 to 25000\n",
      "INFO:__main__:Retrieved 0 total rows from GSC API\n",
      "INFO:__main__:Querying rows 0 to 25000\n",
      "INFO:__main__:Retrieved 0 total rows from GSC API\n",
      "INFO:__main__:Querying data from 2026-01-07 to 2026-02-04\n",
      "INFO:__main__:Querying rows 0 to 25000\n",
      "INFO:__main__:Retrieved 0 total rows from GSC API\n",
      "WARNING:__main__:No data returned from GSC API\n",
      "INFO:__main__:Inferred index coverage for 0 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GSC collection complete\n",
      "Pages: 0\n",
      "Queries: 0\n",
      "Index coverage: 0\n",
      "Sitemaps: 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_collect_gsc_node()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb46b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"GSC_CREDENTIALS_JSON\"] = json.dumps({\n",
    "    \"type\": \"service_account\",\n",
    "    \"project_id\": \"seo-agent-486408\",\n",
    "    \"private_key_id\": \"51492a0c6774672a3e75acf377280e31d9a41713\",\n",
    "    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDyHmPb2Fea4ScV\\nMfbqeQHSfFV+A8reH6V/r8S+bDiRKzZPaIdGrkFrd/pj7oaTnpEgqQeMB4GSsyc3\\nIh6dNDOjly2LfUdcxG3k66fze5SsiQlNiw6qJb29fQG4OXaJYPwLfyOuUzLs25gq\\n6sZEkJ3Far48cswy0x+MB2AwqXaR7dQ7F1JpSAfdIoF+cO3eO+G2pxI2Ehg0SmNb\\nw+y3ICeapiojT7sLrRYYsy/4MJxctVyn3RGA4lE3k3ZXperfsw3jmRkQ7laf7nwZ\\nn1xkhMuf3dBrBQrKfEzsEq5kF3ePoPJALscbSQIvMMStpCiEtK5SlcsOJ4qOItw4\\neLqCxtCxAgMBAAECggEAHui3SdUcFfchZKjMw1EwZN0fkudMCkBTAfJ/9Ole7VVt\\naQTIpELRsjUOX5yPTKAlGdzKTFuW9JHvcy/lZZ9I6lz94P1c02B2QQ++kKxZpg8W\\ng3I5rIoF539ibVDUq6QYeSVBels/uJIuImh3aEBoC9BdOd1T6Tcfo2I7quqvauCI\\nsmfvpAP3LVZ0ODlduUBovVavISA48yOQfmed2uQCuMihj35NrJ/qZItMQEkELZfa\\nWfJFiyBM9fMcVfFS3tQh1sQsoNKnSDovrFGh4wtOMSqPlseeAcnjZasX/0Q1rjo6\\nGZ3COaETJ83g+6wXNDciHa3AHYhd1sJ5OL8AMOGMWQKBgQD7Gtn5d09D9Uiv6q8U\\nH8MEj6TanRcXLR0Lb1MRmjilI2lJNmSs0AofuxCqcMW6QBzo2eeDeeU7O5a5TliI\\njRVRv7xdq/TSBmA3jKlq5Yue57J77VrcIS/Vn1q4Wdro/Ia82Bc6nCtqIeXf6sGN\\nSsALepYHCeNUkCUElBp5uUJOzQKBgQD21rFYB1tNCaCVO2b5ty+JF2vT8esynd93\\niFHxmjVJb94+Ox24myVq1tAyGbMM3o6Cd5t1agZHKc5P6+R72i51mVf8IANSTM0S\\n2+UP57xafsZJcyXh0RhKVXhPzdwkwKp8CDd/iqhBoYZKj1sDnj8KdH9tgcATkLc8\\nzvBUtdIBdQKBgQDXI3xaJoS9PuxV/IaggU3/HGsr+qeL7dUStQDA9hdONXbBiV7c\\nSSpDYWy3+wMNvlyGjBu1I7zo8PcEMUHdTLNVZScoQmnxgBDzxwpoUd87+FuNniDY\\ntX5cUrnRdPr/30w5hBLy3la8Cer/3AyU19SOgSsFQM2K6C0vLQ05+SX8iQKBgDLL\\nLyc8LE+IAaFz3dbZGVEOnsO4bnu1/Pwt8x7SatC30nXf2FfapTJ9Dc6hdzTYv16d\\nmkHpFW2jbq8HjbbmyNSuP2rEc6qF31VYJqTFPP8qQ0duCIeVbXgualmRtRFjDDIF\\nxtXaBJGs2WOSaFowQCy4mFhCcPIVv38l3lKwL4zZAoGBAJR3N4Y/6uyI57vUp6mI\\nWsYZASmLwNyNdkgYOaJYPrtuhqNEosb7MmkLsIbsm2iw7g7W9H5t7y49mwRg3Lc1\\nZpvx3CMWvXHvMgxS3vMfbcA82gCQvz0Cb7J9i7I1Kbqw9yg4LjFXmKhWNb/sUKEW\\nIU9J1MJKHT38qZwpY56zUrD4\\n-----END PRIVATE KEY-----\\n\",\n",
    "    \"client_email\": \"gsc-data-collection@seo-agent-486408.iam.gserviceaccount.com\",\n",
    "    \"client_id\": \"111419250013032075846\",\n",
    "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gsc-data-collection%40seo-agent-486408.iam.gserviceaccount.com\",\n",
    "    \"universe_domain\": \"googleapis.com\"\n",
    "})\n",
    "\n",
    "\n",
    "# If collect_gsc is in another module, import it like:\n",
    "# from your_module.gsc_node import collect_gsc\n",
    "\n",
    "def run_collect_gsc_node():\n",
    "    # --- Option A: credentials via env var (JSON string) ---\n",
    "    # os.environ[\"GSC_CREDENTIALS_JSON\"] = json.dumps({...service_account_json...})\n",
    "\n",
    "    # --- Option B: credentials via file path (recommended locally) ---\n",
    "    # os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/absolute/path/to/service_account.json\"\n",
    "    \n",
    "    credentials_path = \"C:/path/to/your/actual/credentials.json\"  # Use forward slashes\n",
    "    service = authenticate_gsc(credentials_path)\n",
    "\n",
    "    state = {\n",
    "        \"run\": {\n",
    "            \"domain\": \"paddleaurum.com\"\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"integrations\": {\n",
    "                # URL-prefix property example (keep trailing slash or node will add it)\n",
    "                \"gsc_property_url\": \"https://www.paddleaurum.com/\",\n",
    "\n",
    "                # Or domain property example:\n",
    "                # \"gsc_property_url\": \"sc-domain:example.com\",\n",
    "\n",
    "                # Credentials source: can be dict, json string, base64 json, or path\n",
    "                # If you don't set this, node falls back to env vars:\n",
    "                #   GSC_CREDENTIALS_JSON / GSC_CREDENTIALS_B64 / GOOGLE_APPLICATION_CREDENTIALS\n",
    "                \"gsc_credentials\": json.loads(os.environ[\"GSC_CREDENTIALS_JSON\"]),\n",
    "\n",
    "                # Optional tuning\n",
    "                \"gsc_window_days\": 28,\n",
    "                \"gsc_search_type\": \"web\",         # web|news|video|image|discover|googleNews\n",
    "                \"gsc_row_limit\": 50_000,          # cap per report (page+appearance, query+appearance)\n",
    "                \"gsc_max_pages\": 10_000,\n",
    "                \"gsc_max_queries\": 10_000,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    patch = collect_gsc(state)\n",
    "    gsc = patch[\"inputs\"][\"gsc\"]\n",
    "\n",
    "    print(\"✅ collect_gsc_node finished\")\n",
    "    print(\"Collected at:\", gsc[\"collected_at\"])\n",
    "    print(\"Window days:\", gsc[\"window_days\"])\n",
    "    print(\"Pages:\", len(gsc[\"pages\"]))\n",
    "    print(\"Queries:\", len(gsc[\"queries\"]))\n",
    "    print(\"Index coverage:\", len(gsc[\"index_coverage\"]))\n",
    "    print(\"Sitemaps:\", len(gsc[\"sitemap_status\"]))\n",
    "\n",
    "    # quick peek\n",
    "    if gsc[\"pages\"]:\n",
    "        print(\"\\n--- sample page ---\")\n",
    "        print(json.dumps(gsc[\"pages\"][0], indent=2))\n",
    "\n",
    "    if gsc[\"queries\"]:\n",
    "        print(\"\\n--- sample query ---\")\n",
    "        print(json.dumps(gsc[\"queries\"][0], indent=2))\n",
    "\n",
    "    return patch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_collect_gsc_node()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "df60bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from pathlib import Path\n",
    "\n",
    "# sa_path = Path(r\"C:\\real\\path\\to\\service_account.json\")\n",
    "# os.environ[\"GSC_CREDENTIALS_JSON\"] = sa_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def run_collect_gsc_node():\n",
    "    credentials_path = Path(r\"C:\\Users\\Aurum\\vscode\\E-commerce-SEO-Agent\\secret\\seo-agent-486408-51492a0c6774.json\")\n",
    "    if not credentials_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing credentials: {credentials_path}\")\n",
    "\n",
    "    state = {\n",
    "        \"run\": {\"domain\": \"paddleaurum.com\"},\n",
    "        \"config\": {\n",
    "            \"integrations\": {\n",
    "                \"gsc_property_url\": \"sc-domain:paddleaurum.com\",\n",
    "                \"gsc_credentials\": str(credentials_path),  # pass the path string\n",
    "                \"gsc_window_days\": 28,\n",
    "                \"gsc_search_type\": \"web\",\n",
    "                \"gsc_row_limit\": 50_000,\n",
    "                \"gsc_max_pages\": 10_000,\n",
    "                \"gsc_max_queries\": 10_000,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    patch = collect_gsc(state)\n",
    "    print(\"Collected pages:\", len(patch[\"inputs\"][\"gsc\"][\"pages\"]))\n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8da4d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting GSC data collection\n",
      "INFO:__main__:Initializing GSC client for property: sc-domain:paddleaurum.com\n",
      "INFO:__main__:Loading credentials from file: C:\\Users\\Aurum\\vscode\\E-commerce-SEO-Agent\\secret\\seo-agent-486408-51492a0c6774.json\n",
      "INFO:googleapiclient.discovery_cache:file_cache is only supported with oauth2client<4.0.0\n",
      "INFO:__main__:✓ Authenticated to GSC for property: sc-domain:paddleaurum.com\n",
      "ERROR:__main__:✗ GSC collection failed: name 'search_type' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Aurum\\AppData\\Local\\Temp\\ipykernel_3372\\3535914434.py\", line 672, in collect_gsc\n",
      "    search_type=search_type\n",
      "NameError: name 'search_type' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected pages: 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_collect_gsc_node()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "351a3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "credentials_path = Path(r\"C:\\Users\\Aurum\\vscode\\E-commerce-SEO-Agent\\secret\\seo-agent-486408-51492a0c6774.json\")\n",
    "service = authenticate_gsc(str(credentials_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5771fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'siteEntry': [{'siteUrl': 'sc-domain:paddleaurum.com',\n",
       "   'permissionLevel': 'siteOwner'}]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service.sites().list().execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48bfed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_gsc_access(service):\n",
    "    sites = service.sites().list().execute()\n",
    "    for s in sites.get(\"siteEntry\", []):\n",
    "        print(f\"{s['siteUrl']} → {s['permissionLevel']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51f70bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc-domain:paddleaurum.com → siteOwner\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    debug_gsc_access(service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7999483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
