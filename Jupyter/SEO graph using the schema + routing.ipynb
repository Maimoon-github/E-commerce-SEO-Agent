{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bc173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:937: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `JsonSpec` to V2.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:937: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `JsonSpec` to V2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# from langgraph.graph import StateGraph, START, END\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END, START\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# from langchain_core.tools import BaseTool, tool\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\langchain\\agents\\__init__.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     create_json_agent,\n\u001b[0;32m     36\u001b[0m     create_openapi_agent,\n\u001b[0;32m     37\u001b[0m     create_pbi_agent,\n\u001b[0;32m     38\u001b[0m     create_pbi_chat_agent,\n\u001b[0;32m     39\u001b[0m     create_spark_sql_agent,\n\u001b[0;32m     40\u001b[0m     create_sql_agent,\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m as_import_path\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     Agent,\n\u001b[0;32m     46\u001b[0m     AgentExecutor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     LLMSingleActionAgent,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\langchain_community\\agent_toolkits\\__init__.py:165\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _module_lookup:\n\u001b[1;32m--> 165\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_module_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\langchain_community\\agent_toolkits\\openapi\\base.py:13\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     OPENAPI_PREFIX,\n\u001b[0;32m     11\u001b[0m     OPENAPI_SUFFIX,\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoolkit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAPIToolkit\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\langchain_community\\agent_toolkits\\openapi\\toolkit.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JsonSpec\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     RequestsDeleteTool,\n\u001b[0;32m     17\u001b[0m     RequestsGetTool,\n\u001b[0;32m     18\u001b[0m     RequestsPatchTool,\n\u001b[0;32m     19\u001b[0m     RequestsPostTool,\n\u001b[0;32m     20\u001b[0m     RequestsPutTool,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextRequestsWrapper\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRequestsToolkit\u001b[39;00m(BaseToolkit):\n",
      "File \u001b[1;32mc:\\Users\\Aurum\\miniconda3\\envs\\seo_env\\lib\\site-packages\\langchain_community\\tools\\requests\\tool.py:49\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     37\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set allow_dangerous_requests to True to use this tool. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequests can be dangerous and can lead to security vulnerabilities. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfurther security information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m             )\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRequestsGetTool\u001b[39;00m(BaseRequestsTool, BaseTool):\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Tool for making a GET request to an API endpoint.\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequests_get\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Step 0: Imports, type aliases, and shared utilities\n",
    "# ============================================================\n",
    " \n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Literal, TypedDict, Callable\n",
    "\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# LangChain / LangGraph / CrewAI imports\n",
    "# (Make sure these are installed in your environment)\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "# from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain.agents import AgentExecutor\n",
    "# from langchain_core.tools import BaseTool, tool\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "try:\n",
    "    from crewai.flow import Flow, start, listen\n",
    "except ImportError:\n",
    "    # Optional: you can comment this out if CrewAI isn't installed yet\n",
    "    Flow = object  # type: ignore\n",
    "    def start():\n",
    "        def _wrap(fn): return fn\n",
    "        return _wrap\n",
    "    def listen(*_args, **_kwargs):\n",
    "        def _wrap(fn): return fn\n",
    "        return _wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e618c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Core imports successful\n"
     ]
    }
   ],
   "source": [
    "# Import Core Libraries\n",
    "from typing import TypedDict, List, Dict, Optional, Any, Callable, Union\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.tools import tool, StructuredTool, Tool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import Agent, AgentExecutor\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "\n",
    "print(\"✓ Core imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e8365",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Step 1: Load & parse design files\n",
    "# (You can use this to introspect the original .md specs if needed)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eabd61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_design_files(base_path: Path | str = Path(\".\")) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Implement basic file loading for the two design docs.\n",
    "\n",
    "    This keeps the notebook self-contained while still letting you\n",
    "    inspect the original schema/layout/graph contracts if you want\n",
    "    to programmatically derive anything later.\n",
    "    \"\"\"\n",
    "    base_path = Path(base_path)\n",
    "    files = {\n",
    "        \"schema_layout_contracts\": base_path / \"Schema, Layout, Contracts.md\",\n",
    "        \"universal_graph\": base_path / \"Universal Graph.md\",\n",
    "    }\n",
    "\n",
    "    contents: Dict[str, str] = {}\n",
    "    for key, path in files.items():\n",
    "        try:\n",
    "            contents[key] = path.read_text(encoding=\"utf-8\")\n",
    "        except FileNotFoundError:\n",
    "            # It's fine if the files aren't present in this environment;\n",
    "            # the graph wiring below is already baked from the spec.\n",
    "            contents[key] = \"\"\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b62c8",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Step 2: Define universal state + tools abstraction\n",
    "# (Minimal but structurally aligned with your design)\n",
    "# ============================================================\n",
    "\n",
    "# --- 2.1 State shape (TypedDict for LangGraph type checking) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a9c19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PlanState(TypedDict, total=False):\n",
    "    work_queue: List[str]\n",
    "    in_progress: Optional[str]\n",
    "    changesets: List[Dict[str, Any]]\n",
    "    tickets: List[Dict[str, Any]]\n",
    "    plan_created_at: Optional[str]  # iso8601\n",
    "\n",
    "\n",
    "class ApprovalsState(TypedDict, total=False):\n",
    "    required: bool\n",
    "    status: str  # ApprovalStatus in your schema\n",
    "    decisions: List[Dict[str, Any]]\n",
    "\n",
    "\n",
    "class ExecutionState(TypedDict, total=False):\n",
    "    applied_changesets: List[str]\n",
    "    failed_changesets: List[str]\n",
    "    skipped_changesets: List[str]\n",
    "    execution_notes: List[str]\n",
    "\n",
    "\n",
    "class ValidationState(TypedDict, total=False):\n",
    "    pre_metrics: Dict[str, Any]\n",
    "    post_metrics: Dict[str, Any]\n",
    "    checks: List[Dict[str, Any]]\n",
    "    validated_at: Optional[str]\n",
    "\n",
    "\n",
    "class SEOState(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    Design this as a thin shell over your full AurumSEOState Pydantic model.\n",
    "\n",
    "    Top-level keys directly mirror the schema in the design doc:\n",
    "      - run, config, inputs, findings, scores, plan, approvals, execution,\n",
    "        validation, audit_log, errors.\n",
    "    The nested structures are intentionally loose here so you can swap in\n",
    "    the full Pydantic models without changing graph wiring.\n",
    "    \"\"\"\n",
    "    run: Dict[str, Any]\n",
    "    config: Dict[str, Any]\n",
    "    inputs: Dict[str, Any]\n",
    "    findings: Dict[str, Any]\n",
    "    scores: Dict[str, Any]\n",
    "    plan: PlanState\n",
    "    approvals: ApprovalsState\n",
    "    execution: ExecutionState\n",
    "    validation: ValidationState\n",
    "    audit_log: List[Dict[str, Any]]\n",
    "    errors: List[Dict[str, Any]]\n",
    "\n",
    "\n",
    "def ensure_default_state(state: Optional[SEOState] = None) -> SEOState:\n",
    "    \"\"\"\n",
    "    Ensure the state has all required top-level containers.\n",
    "\n",
    "    This lets you start from {} and still get a structurally valid state,\n",
    "    while remaining compatible with your richer Pydantic AurumSEOState\n",
    "    if you plug that in instead.\n",
    "    \"\"\"\n",
    "    if state is None:\n",
    "        state = SEOState()  # type: ignore[call-arg]\n",
    "\n",
    "    state.setdefault(\"run\", {})\n",
    "    state.setdefault(\"config\", {})\n",
    "    state.setdefault(\"inputs\", {})\n",
    "    state.setdefault(\"findings\", {})\n",
    "    state.setdefault(\"scores\", {})\n",
    "    state.setdefault(\"plan\", {})\n",
    "    state.setdefault(\"approvals\", {})\n",
    "    state.setdefault(\"execution\", {})\n",
    "    state.setdefault(\"validation\", {})\n",
    "    state.setdefault(\"audit_log\", [])\n",
    "    state.setdefault(\"errors\", [])\n",
    "\n",
    "    plan: PlanState = state[\"plan\"]  # type: ignore[assignment]\n",
    "    plan.setdefault(\"work_queue\", [])\n",
    "    plan.setdefault(\"in_progress\", None)\n",
    "    plan.setdefault(\"changesets\", [])\n",
    "    plan.setdefault(\"tickets\", [])\n",
    "    plan.setdefault(\"plan_created_at\", None)\n",
    "\n",
    "    approvals: ApprovalsState = state[\"approvals\"]  # type: ignore[assignment]\n",
    "    approvals.setdefault(\"required\", False)\n",
    "    approvals.setdefault(\"status\", \"NOT_REQUIRED\")\n",
    "    approvals.setdefault(\"decisions\", [])\n",
    "\n",
    "    execution: ExecutionState = state[\"execution\"]  # type: ignore[assignment]\n",
    "    execution.setdefault(\"applied_changesets\", [])\n",
    "    execution.setdefault(\"failed_changesets\", [])\n",
    "    execution.setdefault(\"skipped_changesets\", [])\n",
    "    execution.setdefault(\"execution_notes\", [])\n",
    "\n",
    "    validation: ValidationState = state[\"validation\"]  # type: ignore[assignment]\n",
    "    validation.setdefault(\"pre_metrics\", {})\n",
    "    validation.setdefault(\"post_metrics\", {})\n",
    "    validation.setdefault(\"checks\", [])\n",
    "    validation.setdefault(\"validated_at\", None)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b37ef2",
   "metadata": {},
   "source": [
    "# --- 2.2 LangChain tools bundle for SEO connectors ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f5ab58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_gsc_snapshot(domain: str) -> Dict[str, Any]:\n",
    "    \"\"\"Implement: Fetch a GSC snapshot for the given domain (placeholder).\"\"\"\n",
    "    raise NotImplementedError(\"Wire this to your GSC connector.\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_ga4_snapshot(property_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Implement: Fetch a GA4 snapshot (placeholder).\"\"\"\n",
    "    raise NotImplementedError(\"Wire this to your GA4 connector.\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_serp_snapshot(keywords: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Implement: Fetch a SERP snapshot (placeholder).\"\"\"\n",
    "    raise NotImplementedError(\"Wire this to your SERP provider.\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_crawl_diff(domain: str) -> Dict[str, Any]:\n",
    "    \"\"\"Implement: Fetch crawl diff from Screaming Frog / custom crawler (placeholder).\"\"\"\n",
    "    raise NotImplementedError(\"Wire this to your crawler output.\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def apply_cms_changes(changeset: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Implement: Apply a ChangeSet to your CMS (placeholder).\"\"\"\n",
    "    raise NotImplementedError(\"Wire this to Shopify/Woo/Headless CMS.\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_ticket(payload: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Implement: Create a ticket in Jira/Asana/etc (placeholder).\"\"\"\n",
    "    raise NotImplementedError(\"Wire this to your ticketing system.\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SEOAgentTools:\n",
    "    \"\"\"\n",
    "    Design: thin container for all LangChain tools used by the graph nodes.\n",
    "    You can inject different implementations in tests vs production.\n",
    "    \"\"\"\n",
    "    gsc: BaseTool = fetch_gsc_snapshot\n",
    "    ga4: BaseTool = fetch_ga4_snapshot\n",
    "    serp: BaseTool = fetch_serp_snapshot\n",
    "    crawl: BaseTool = fetch_crawl_diff\n",
    "    cms: BaseTool = apply_cms_changes\n",
    "    ticketing: BaseTool = create_ticket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e0c30",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Step 3: Implement reusable node functions (universal core)\n",
    "# Each node: SEOState × SEOAgentTools -> SEOState\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f54ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _append_audit(state: SEOState, node: str, event: str, detail: Dict[str, Any]) -> None:\n",
    "    state[\"audit_log\"].append(\n",
    "        {\n",
    "            \"ts\": datetime.utcnow().isoformat(),\n",
    "            \"node\": node,\n",
    "            \"event\": event,\n",
    "            \"detail\": detail,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd276e3a",
   "metadata": {},
   "source": [
    "# ------------ Observe phase nodes ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c30d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_collect_gsc(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    domain = state[\"run\"].get(\"domain\", \"https://aurumpickleball.com\")\n",
    "    # Placeholder: real implementation should call tools.gsc.invoke\n",
    "    state[\"inputs\"][\"gsc\"] = {\n",
    "        \"collected_at\": datetime.utcnow().isoformat(),\n",
    "        \"window_days\": 28,\n",
    "        \"pages\": [],\n",
    "        \"queries\": [],\n",
    "        \"index_coverage\": [],\n",
    "        \"sitemap_status\": [],\n",
    "        \"domain\": domain,\n",
    "    }\n",
    "    _append_audit(state, \"collect_gsc\", \"END\", {\"domain\": domain})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_collect_ga4(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    property_id = state.get(\"config\", {}).get(\"integrations\", {}).get(\"ga4_property_id\", \"UNSET\")\n",
    "    state[\"inputs\"][\"ga4\"] = {\n",
    "        \"collected_at\": datetime.utcnow().isoformat(),\n",
    "        \"window_days\": 28,\n",
    "        \"landing_pages\": [],\n",
    "        \"conversions\": [],\n",
    "        \"property_id\": property_id,\n",
    "    }\n",
    "    _append_audit(state, \"collect_ga4\", \"END\", {\"property_id\": property_id})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_collect_serp_snapshot(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    keywords = []  # In practice: derive from GSC queries\n",
    "    state[\"inputs\"][\"serp\"] = {\n",
    "        \"collected_at\": datetime.utcnow().isoformat(),\n",
    "        \"keywords\": [],\n",
    "        \"competitors\": [],\n",
    "    }\n",
    "    _append_audit(state, \"collect_serp_snapshot\", \"END\", {\"keywords_seeded\": len(keywords)})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_collect_crawl_diff(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    domain = state[\"run\"].get(\"domain\", \"https://aurumpickleball.com\")\n",
    "    state[\"inputs\"][\"crawl\"] = {\n",
    "        \"collected_at\": datetime.utcnow().isoformat(),\n",
    "        \"tool\": \"screamingfrog\",\n",
    "        \"pages\": [],\n",
    "        \"diffs\": [],\n",
    "        \"domain\": domain,\n",
    "    }\n",
    "    _append_audit(state, \"collect_crawl_diff\", \"END\", {\"domain\": domain})\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97078285",
   "metadata": {},
   "source": [
    "# ------------ Diagnose phase nodes ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6590185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_normalize_inputs(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    # Design: normalization is conceptual here; real implementation should\n",
    "    # canonicalize shapes according to your schema contract doc.\n",
    "    _append_audit(state, \"normalize_inputs\", \"END\", {\"normalized\": True})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_detect_anomalies(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    # Design: in a full build, you'd compare inputs.gsc / inputs.ga4 against\n",
    "    # previous baselines and record anomalies. Here we just log a stub.\n",
    "    _append_audit(state, \"detect_anomalies\", \"END\", {\"anomaly_scan\": \"stub\"})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_classify_issues(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    findings = state.setdefault(\"findings\", {})\n",
    "    issues: List[Dict[str, Any]] = findings.get(\"issues\", [])\n",
    "    if not issues:\n",
    "        # Implement: create at least one stub issue so the downstream flow has something to route\n",
    "        issue_id = \"issue_stub_001\"\n",
    "        issues.append(\n",
    "            {\n",
    "                \"issue_id\": issue_id,\n",
    "                \"issue_type\": \"CTR_DROP\",\n",
    "                \"market\": \"US\",\n",
    "                \"url\": \"https://aurumpickleball.com/collections/paddles\",\n",
    "                \"severity\": 3,\n",
    "                \"evidence\": {},\n",
    "                \"detected_at\": datetime.utcnow().isoformat(),\n",
    "                \"owner_hint\": \"PRODUCT\",\n",
    "            }\n",
    "        )\n",
    "        findings[\"issues\"] = issues\n",
    "    _append_audit(state, \"classify_issues\", \"END\", {\"issues\": len(issues)})\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec3d0b",
   "metadata": {},
   "source": [
    "# ------------ Recommend phase nodes ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c069538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_opportunity(issue: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Implement deterministic opportunity scoring consistent with the spec:\n",
    "      priority_score = (impact * confidence) / max(effort, 1)\n",
    "    For now, we stub a simple mapping and default score.\n",
    "    \"\"\"\n",
    "    opportunity_id = f\"opp_{issue['issue_id']}\"\n",
    "    return {\n",
    "        \"opportunity_id\": opportunity_id,\n",
    "        \"opportunity_type\": issue.get(\"owner_hint\", \"TECH\"),\n",
    "        \"market\": issue.get(\"market\", \"US\"),\n",
    "        \"primary_url\": issue.get(\"url\"),\n",
    "        \"related_urls\": [],\n",
    "        \"keyword_cluster\": [],\n",
    "        \"issue_refs\": [issue[\"issue_id\"]],\n",
    "        \"evidence\": {},\n",
    "        \"impact\": 3,\n",
    "        \"confidence\": 3,\n",
    "        \"effort\": 2,\n",
    "        \"risk\": 2,\n",
    "        \"priority_score\": (3 * 3) / 2,\n",
    "        \"rationale\": \"stub scoring\",\n",
    "    }\n",
    "\n",
    "\n",
    "def node_score_opportunities(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    findings = state.setdefault(\"findings\", {})\n",
    "    issues: List[Dict[str, Any]] = findings.get(\"issues\", [])\n",
    "\n",
    "    opportunities: List[Dict[str, Any]] = []\n",
    "    for issue in issues:\n",
    "        opportunities.append(_score_opportunity(issue))\n",
    "\n",
    "    # Deterministic sort:\n",
    "    opportunities.sort(\n",
    "        key=lambda o: (\n",
    "            -o[\"priority_score\"],\n",
    "            -o[\"impact\"],\n",
    "            -o[\"confidence\"],\n",
    "            o[\"effort\"],\n",
    "            o[\"primary_url\"] or \"\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    scores = state.setdefault(\"scores\", {})\n",
    "    scores[\"ranked_opportunity_ids\"] = [o[\"opportunity_id\"] for o in opportunities]\n",
    "    scores[\"score_calculated_at\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "    findings[\"opportunities\"] = opportunities\n",
    "\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "    plan[\"work_queue\"] = scores[\"ranked_opportunity_ids\"].copy()\n",
    "    plan[\"plan_created_at\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "    _append_audit(\n",
    "        state,\n",
    "        \"score_opportunities\",\n",
    "        \"END\",\n",
    "        {\"opportunities\": len(opportunities)},\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_route_by_type_and_priority(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    \"\"\"\n",
    "    This node itself doesn't mutate much; routing decisions happen in the\n",
    "    LangGraph conditional edge function. Here we only ensure the state\n",
    "    has a valid 'in_progress' set when the router is invoked directly\n",
    "    (e.g., from CrewAI).\n",
    "    \"\"\"\n",
    "    state = ensure_default_state(state)\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "\n",
    "    if plan[\"in_progress\"] is None and plan[\"work_queue\"]:\n",
    "        next_id = plan[\"work_queue\"].pop(0)\n",
    "        plan[\"in_progress\"] = next_id\n",
    "\n",
    "    _append_audit(\n",
    "        state,\n",
    "        \"route_by_type_and_priority\",\n",
    "        \"END\",\n",
    "        {\"in_progress\": state[\"plan\"][\"in_progress\"]},\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b37cf",
   "metadata": {},
   "source": [
    "# ------------ Specialist nodes (ChangeSet generators) ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc6a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _append_changeset(\n",
    "    state: SEOState,\n",
    "    opportunity_id: str,\n",
    "    kind: str,\n",
    "    target_url: str,\n",
    "    risk: int = 2,\n",
    ") -> None:\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "    changesets = plan[\"changesets\"]\n",
    "    cs_id = f\"cs_{kind.lower()}_{len(changesets) + 1:03d}\"\n",
    "    changesets.append(\n",
    "        {\n",
    "            \"changeset_id\": cs_id,\n",
    "            \"kind\": kind,\n",
    "            \"opportunity_id\": opportunity_id,\n",
    "            \"market\": \"US\",\n",
    "            \"approval_required\": True,\n",
    "            \"approval_status\": \"PENDING\",\n",
    "            \"target\": {\"url\": target_url},\n",
    "            \"operations\": [\n",
    "                {\n",
    "                    \"op_id\": f\"op_{cs_id}_01\",\n",
    "                    \"op_type\": \"UPDATE_META\",\n",
    "                    \"target\": {\"url\": target_url},\n",
    "                    \"payload\": {},\n",
    "                    \"rollback\": {},\n",
    "                    \"qa_checks\": [\"meta_length_ok\", \"no_indexing_issues\"],\n",
    "                    \"risk\": risk,\n",
    "                }\n",
    "            ],\n",
    "            \"predicted_impact\": {},\n",
    "            \"rollback_plan\": {},\n",
    "            \"notes\": \"stub changeset\",\n",
    "            \"execution_status\": \"QUEUED\",\n",
    "            \"executed_at\": None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_current_opportunity(state: SEOState) -> Optional[Dict[str, Any]]:\n",
    "    findings = state.get(\"findings\", {})\n",
    "    opps: List[Dict[str, Any]] = findings.get(\"opportunities\", [])\n",
    "    in_progress = state[\"plan\"][\"in_progress\"]\n",
    "    if not in_progress:\n",
    "        return None\n",
    "    for o in opps:\n",
    "        if o[\"opportunity_id\"] == in_progress:\n",
    "            return o\n",
    "    return None\n",
    "\n",
    "\n",
    "def node_techseo_agent_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    opp = _get_current_opportunity(state)\n",
    "    if opp:\n",
    "        _append_changeset(\n",
    "            state,\n",
    "            opportunity_id=opp[\"opportunity_id\"],\n",
    "            kind=\"TECH_FIX\",\n",
    "            target_url=opp[\"primary_url\"],\n",
    "            risk=3,\n",
    "        )\n",
    "    state[\"plan\"][\"in_progress\"] = None\n",
    "    _append_audit(state, \"techseo_agent_node\", \"END\", {\"changesets\": len(state[\"plan\"][\"changesets\"])})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_productseo_agent_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    opp = _get_current_opportunity(state)\n",
    "    if opp:\n",
    "        _append_changeset(\n",
    "            state,\n",
    "            opportunity_id=opp[\"opportunity_id\"],\n",
    "            kind=\"PRODUCT_PDP_UPDATE\",\n",
    "            target_url=opp[\"primary_url\"],\n",
    "            risk=2,\n",
    "        )\n",
    "    state[\"plan\"][\"in_progress\"] = None\n",
    "    _append_audit(state, \"productseo_agent_node\", \"END\", {\"changesets\": len(state[\"plan\"][\"changesets\"])})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_content_agent_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    opp = _get_current_opportunity(state)\n",
    "    if opp:\n",
    "        _append_changeset(\n",
    "            state,\n",
    "            opportunity_id=opp[\"opportunity_id\"],\n",
    "            kind=\"CONTENT_BRIEF\",\n",
    "            target_url=opp[\"primary_url\"],\n",
    "            risk=2,\n",
    "        )\n",
    "    state[\"plan\"][\"in_progress\"] = None\n",
    "    _append_audit(state, \"content_agent_node\", \"END\", {\"changesets\": len(state[\"plan\"][\"changesets\"])})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_authority_agent_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    opp = _get_current_opportunity(state)\n",
    "    if opp:\n",
    "        _append_changeset(\n",
    "            state,\n",
    "            opportunity_id=opp[\"opportunity_id\"],\n",
    "            kind=\"AUTHORITY_OUTREACH_PLAN\",\n",
    "            target_url=opp[\"primary_url\"],\n",
    "            risk=1,\n",
    "        )\n",
    "    state[\"plan\"][\"in_progress\"] = None\n",
    "    _append_audit(state, \"authority_agent_node\", \"END\", {\"changesets\": len(state[\"plan\"][\"changesets\"])})\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ea68e",
   "metadata": {},
   "source": [
    "# ------------ Approve / Execute / Validate nodes ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "721ac4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_qa_guardrail_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "    approvals: ApprovalsState = state[\"approvals\"]\n",
    "\n",
    "    risk_cfg = state.get(\"config\", {}).get(\"risk\", {})\n",
    "    approval_threshold = risk_cfg.get(\"approval_threshold\", 4)\n",
    "    auto_execute_max_risk = risk_cfg.get(\"auto_execute_max_risk\", 2)\n",
    "\n",
    "    requires_approval = False\n",
    "    has_high_risk = False\n",
    "\n",
    "    for cs in plan[\"changesets\"]:\n",
    "        max_risk = max((op.get(\"risk\", 0) for op in cs.get(\"operations\", [])), default=0)\n",
    "        if max_risk >= approval_threshold:\n",
    "            cs[\"approval_required\"] = True\n",
    "            cs[\"approval_status\"] = \"PENDING\"\n",
    "            requires_approval = True\n",
    "            has_high_risk = True\n",
    "        elif max_risk <= auto_execute_max_risk:\n",
    "            cs[\"approval_required\"] = False\n",
    "            cs[\"approval_status\"] = \"NOT_REQUIRED\"\n",
    "        else:\n",
    "            cs[\"approval_required\"] = True\n",
    "            cs[\"approval_status\"] = \"PENDING\"\n",
    "            requires_approval = True\n",
    "\n",
    "    approvals[\"required\"] = requires_approval\n",
    "    approvals[\"status\"] = \"PENDING\" if requires_approval or has_high_risk else \"NOT_REQUIRED\"\n",
    "\n",
    "    _append_audit(\n",
    "        state,\n",
    "        \"qa_guardrail_node\",\n",
    "        \"END\",\n",
    "        {\"changesets\": len(plan[\"changesets\"]), \"required\": approvals[\"required\"]},\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_approval_interrupt_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    \"\"\"\n",
    "    In a real system this node would be paired with LangGraph interrupts\n",
    "    and/or CrewAI HITL to actually wait for human approval.\n",
    "\n",
    "    Here we assume `approvals.decisions` has been populated externally\n",
    "    and we simply aggregate them into final statuses.\n",
    "    \"\"\"\n",
    "    state = ensure_default_state(state)\n",
    "    approvals: ApprovalsState = state[\"approvals\"]\n",
    "    decisions = approvals.get(\"decisions\", [])\n",
    "\n",
    "    decisions_by_cs: Dict[str, str] = {d[\"changeset_id\"]: d[\"decision\"] for d in decisions}\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "\n",
    "    statuses: List[str] = []\n",
    "    for cs in plan[\"changesets\"]:\n",
    "        cs_id = cs[\"changeset_id\"]\n",
    "        decision = decisions_by_cs.get(cs_id)\n",
    "        if not decision:\n",
    "            continue\n",
    "        cs[\"approval_status\"] = decision\n",
    "        statuses.append(decision)\n",
    "\n",
    "    if not statuses:\n",
    "        approvals[\"status\"] = \"PENDING\"\n",
    "    elif all(s == \"APPROVED\" for s in statuses):\n",
    "        approvals[\"status\"] = \"APPROVED\"\n",
    "    elif all(s == \"REJECTED\" for s in statuses):\n",
    "        approvals[\"status\"] = \"REJECTED\"\n",
    "    else:\n",
    "        approvals[\"status\"] = \"PARTIAL\"\n",
    "\n",
    "    _append_audit(\n",
    "        state,\n",
    "        \"approval_interrupt_node\",\n",
    "        \"END\",\n",
    "        {\"status\": approvals[\"status\"], \"decisions\": len(decisions)},\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_execute_changeset_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "    approvals: ApprovalsState = state[\"approvals\"]\n",
    "    execution: ExecutionState = state[\"execution\"]\n",
    "\n",
    "    risk_cfg = state.get(\"config\", {}).get(\"risk\", {})\n",
    "    blocklist_kinds = risk_cfg.get(\"blocklist_kinds\", [])\n",
    "\n",
    "    for cs in plan[\"changesets\"]:\n",
    "        cs_id = cs[\"changeset_id\"]\n",
    "        if cs[\"kind\"] in blocklist_kinds:\n",
    "            cs[\"execution_status\"] = \"SKIPPED\"\n",
    "            execution[\"skipped_changesets\"].append(cs_id)\n",
    "            continue\n",
    "\n",
    "        status = cs.get(\"approval_status\", \"NOT_REQUIRED\")\n",
    "        if status not in (\"APPROVED\", \"NOT_REQUIRED\"):\n",
    "            cs[\"execution_status\"] = \"SKIPPED\"\n",
    "            execution[\"skipped_changesets\"].append(cs_id)\n",
    "            continue\n",
    "\n",
    "        # Implement: call CMS tool here in a real build\n",
    "        cs[\"execution_status\"] = \"APPLIED\"\n",
    "        cs[\"executed_at\"] = datetime.utcnow().isoformat()\n",
    "        execution[\"applied_changesets\"].append(cs_id)\n",
    "        execution[\"execution_notes\"].append(f\"Applied {cs_id} (stub).\")\n",
    "\n",
    "    _append_audit(\n",
    "        state,\n",
    "        \"execute_changeset_node\",\n",
    "        \"END\",\n",
    "        {\"applied\": len(execution[\"applied_changesets\"]), \"skipped\": len(execution[\"skipped_changesets\"])},\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_create_tickets_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "    execution: ExecutionState = state[\"execution\"]\n",
    "\n",
    "    for cs in plan[\"changesets\"]:\n",
    "        if cs.get(\"execution_status\") in (\"SKIPPED\", \"FAILED\"):\n",
    "            ticket_id = f\"t_{cs['changeset_id']}\"\n",
    "            plan[\"tickets\"].append(\n",
    "                {\n",
    "                    \"ticket_id\": ticket_id,\n",
    "                    \"system\": \"jira\",\n",
    "                    \"title\": f\"Investigate ChangeSet {cs['changeset_id']}\",\n",
    "                    \"description\": \"Stub ticket based on skipped/failed ChangeSet.\",\n",
    "                    \"priority\": \"P1\",\n",
    "                    \"url_refs\": [cs[\"target\"][\"url\"]],\n",
    "                    \"created_at\": datetime.utcnow().isoformat(),\n",
    "                }\n",
    "            )\n",
    "            execution[\"execution_notes\"].append(\n",
    "                f\"Created ticket {ticket_id} for {cs['changeset_id']}.\"\n",
    "            )\n",
    "\n",
    "    _append_audit(\n",
    "        state,\n",
    "        \"create_tickets_node\",\n",
    "        \"END\",\n",
    "        {\"tickets\": len(plan[\"tickets\"])},\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_validate_metrics_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    validation: ValidationState = state[\"validation\"]\n",
    "    # Implement: compute real before/after metrics in production\n",
    "    validation[\"pre_metrics\"] = validation.get(\"pre_metrics\", {\"org_revenue_28d\": 0})\n",
    "    validation[\"post_metrics\"] = validation.get(\"post_metrics\", {\"org_revenue_28d\": 0})\n",
    "    validation[\"checks\"].append(\n",
    "        {\"name\": \"schema_validates\", \"passed\": True, \"ts\": datetime.utcnow().isoformat()}\n",
    "    )\n",
    "    validation[\"validated_at\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "    _append_audit(state, \"validate_metrics_node\", \"END\", {\"checks\": len(validation[\"checks\"])})\n",
    "    return state\n",
    "\n",
    "\n",
    "def node_persist_run_node(state: SEOState, tools: SEOAgentTools) -> SEOState:\n",
    "    state = ensure_default_state(state)\n",
    "    # Implement: persist to your DB/log; here we just audit.\n",
    "    _append_audit(state, \"persist_run_node\", \"END\", {\"persisted\": True})\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d881af",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Step 4: Construct the LangGraph universal graph\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a580a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _router_condition(state: SEOState) -> str:\n",
    "    \"\"\"\n",
    "    Implement: routing logic for `route_by_type_and_priority` node.\n",
    "\n",
    "    Returns the name of the next node, consistent with the design:\n",
    "      - techseo_agent_node / productseo_agent_node / content_agent_node /\n",
    "        authority_agent_node, or\n",
    "      - qa_guardrail_node if the queue is empty.\n",
    "    \"\"\"\n",
    "    state = ensure_default_state(state)\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "\n",
    "    if not plan[\"work_queue\"]:\n",
    "        return \"qa_guardrail_node\"\n",
    "\n",
    "    # If in_progress isn't set yet, pick the next opportunity and infer type.\n",
    "    if plan[\"in_progress\"] is None:\n",
    "        next_id = plan[\"work_queue\"].pop(0)\n",
    "        plan[\"in_progress\"] = next_id\n",
    "\n",
    "    opp = _get_current_opportunity(state)\n",
    "    opp_type = (opp or {}).get(\"opportunity_type\", \"TECH\")\n",
    "\n",
    "    if opp_type == \"TECH\":\n",
    "        return \"techseo_agent_node\"\n",
    "    if opp_type == \"PRODUCT\":\n",
    "        return \"productseo_agent_node\"\n",
    "    if opp_type == \"CONTENT\":\n",
    "        return \"content_agent_node\"\n",
    "    if opp_type == \"AUTHORITY\":\n",
    "        return \"authority_agent_node\"\n",
    "\n",
    "    # Fallback: send to QA if unknown\n",
    "    return \"qa_guardrail_node\"\n",
    "\n",
    "\n",
    "def _qa_condition(state: SEOState) -> str:\n",
    "    \"\"\"\n",
    "    Implement: routing from `qa_guardrail_node` → approval / execute / tickets.\n",
    "    \"\"\"\n",
    "    state = ensure_default_state(state)\n",
    "    plan: PlanState = state[\"plan\"]\n",
    "    approvals: ApprovalsState = state[\"approvals\"]\n",
    "\n",
    "    if not plan[\"changesets\"]:\n",
    "        return \"create_tickets_node\"\n",
    "\n",
    "    if approvals.get(\"required\", False):\n",
    "        return \"approval_interrupt_node\"\n",
    "\n",
    "    # No approvals required → execute directly\n",
    "    return \"execute_changeset_node\"\n",
    "\n",
    "\n",
    "def _approval_gate_condition(state: SEOState) -> str:\n",
    "    \"\"\"\n",
    "    Implement: routing from `approval_interrupt_node` → execute or tickets.\n",
    "    \"\"\"\n",
    "    state = ensure_default_state(state)\n",
    "    approvals: ApprovalsState = state[\"approvals\"]\n",
    "    status = approvals.get(\"status\", \"PENDING\")\n",
    "\n",
    "    if status in (\"APPROVED\", \"PARTIAL\"):\n",
    "        return \"execute_changeset_node\"\n",
    "    if status == \"REJECTED\":\n",
    "        return \"create_tickets_node\"\n",
    "\n",
    "    # Still pending: in a real system you'd interrupt; here we default to tickets.\n",
    "    return \"create_tickets_node\"\n",
    "\n",
    "\n",
    "def build_langgraph_universal_app(\n",
    "    tools: Optional[SEOAgentTools] = None,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Construct: LangGraph StateGraph wired according to the design doc.\n",
    "    All node functions close over the provided SEOAgentTools bundle.\n",
    "    \"\"\"\n",
    "    if tools is None:\n",
    "        tools = SEOAgentTools()\n",
    "\n",
    "    graph = StateGraph(SEOState)\n",
    "\n",
    "    # Helper to bind tools into node callables\n",
    "    def bind(fn: Callable[[SEOState, SEOAgentTools], SEOState]) -> Callable[[SEOState], SEOState]:\n",
    "        return lambda state: fn(state, tools)\n",
    "\n",
    "    # Register nodes (names must exactly match the design)\n",
    "    graph.add_node(\"collect_gsc\", bind(node_collect_gsc))\n",
    "    graph.add_node(\"collect_ga4\", bind(node_collect_ga4))\n",
    "    graph.add_node(\"collect_serp_snapshot\", bind(node_collect_serp_snapshot))\n",
    "    graph.add_node(\"collect_crawl_diff\", bind(node_collect_crawl_diff))\n",
    "\n",
    "    graph.add_node(\"normalize_inputs\", bind(node_normalize_inputs))\n",
    "    graph.add_node(\"detect_anomalies\", bind(node_detect_anomalies))\n",
    "    graph.add_node(\"classify_issues\", bind(node_classify_issues))\n",
    "    graph.add_node(\"score_opportunities\", bind(node_score_opportunities))\n",
    "    graph.add_node(\"route_by_type_and_priority\", bind(node_route_by_type_and_priority))\n",
    "\n",
    "    graph.add_node(\"techseo_agent_node\", bind(node_techseo_agent_node))\n",
    "    graph.add_node(\"productseo_agent_node\", bind(node_productseo_agent_node))\n",
    "    graph.add_node(\"content_agent_node\", bind(node_content_agent_node))\n",
    "    graph.add_node(\"authority_agent_node\", bind(node_authority_agent_node))\n",
    "\n",
    "    graph.add_node(\"qa_guardrail_node\", bind(node_qa_guardrail_node))\n",
    "    graph.add_node(\"approval_interrupt_node\", bind(node_approval_interrupt_node))\n",
    "    graph.add_node(\"execute_changeset_node\", bind(node_execute_changeset_node))\n",
    "    graph.add_node(\"create_tickets_node\", bind(node_create_tickets_node))\n",
    "    graph.add_node(\"validate_metrics_node\", bind(node_validate_metrics_node))\n",
    "    graph.add_node(\"persist_run_node\", bind(node_persist_run_node))\n",
    "\n",
    "    # Direct edges (Observe → Diagnose → Recommend → Router)\n",
    "    graph.add_edge(START, \"collect_gsc\")\n",
    "    graph.add_edge(\"collect_gsc\", \"collect_ga4\")\n",
    "    graph.add_edge(\"collect_ga4\", \"collect_serp_snapshot\")\n",
    "    graph.add_edge(\"collect_serp_snapshot\", \"collect_crawl_diff\")\n",
    "    graph.add_edge(\"collect_crawl_diff\", \"normalize_inputs\")\n",
    "    graph.add_edge(\"normalize_inputs\", \"detect_anomalies\")\n",
    "    graph.add_edge(\"detect_anomalies\", \"classify_issues\")\n",
    "    graph.add_edge(\"classify_issues\", \"score_opportunities\")\n",
    "    graph.add_edge(\"score_opportunities\", \"route_by_type_and_priority\")\n",
    "\n",
    "    # Conditional routing from router → specialist / QA\n",
    "    graph.add_conditional_edges(\n",
    "        \"route_by_type_and_priority\",\n",
    "        _router_condition,\n",
    "        {\n",
    "            \"techseo_agent_node\": \"techseo_agent_node\",\n",
    "            \"productseo_agent_node\": \"productseo_agent_node\",\n",
    "            \"content_agent_node\": \"content_agent_node\",\n",
    "            \"authority_agent_node\": \"authority_agent_node\",\n",
    "            \"qa_guardrail_node\": \"qa_guardrail_node\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Loop back from specialists → router\n",
    "    graph.add_edge(\"techseo_agent_node\", \"route_by_type_and_priority\")\n",
    "    graph.add_edge(\"productseo_agent_node\", \"route_by_type_and_priority\")\n",
    "    graph.add_edge(\"content_agent_node\", \"route_by_type_and_priority\")\n",
    "    graph.add_edge(\"authority_agent_node\", \"route_by_type_and_priority\")\n",
    "\n",
    "    # QA conditional routing\n",
    "    graph.add_conditional_edges(\n",
    "        \"qa_guardrail_node\",\n",
    "        _qa_condition,\n",
    "        {\n",
    "            \"approval_interrupt_node\": \"approval_interrupt_node\",\n",
    "            \"execute_changeset_node\": \"execute_changeset_node\",\n",
    "            \"create_tickets_node\": \"create_tickets_node\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Approval gate routing\n",
    "    graph.add_conditional_edges(\n",
    "        \"approval_interrupt_node\",\n",
    "        _approval_gate_condition,\n",
    "        {\n",
    "            \"execute_changeset_node\": \"execute_changeset_node\",\n",
    "            \"create_tickets_node\": \"create_tickets_node\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Execute → Tickets → Validate → Persist → END\n",
    "    graph.add_edge(\"execute_changeset_node\", \"create_tickets_node\")\n",
    "    graph.add_edge(\"create_tickets_node\", \"validate_metrics_node\")\n",
    "    graph.add_edge(\"validate_metrics_node\", \"persist_run_node\")\n",
    "    graph.add_edge(\"persist_run_node\", END)\n",
    "\n",
    "    # Compile the graph into an app\n",
    "    app = graph.compile()\n",
    "    return app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ab2e6",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Step 5: Construct the CrewAI Flow adapter using the same nodes\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3c8ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AurumSEO_Flow(Flow):\n",
    "    \"\"\"\n",
    "    Design: CrewAI Flow that reuses the exact same node functions\n",
    "    as the LangGraph app. This gives you a single source of truth\n",
    "    for business logic, with two orchestration front-ends.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tools: Optional[SEOAgentTools] = None):\n",
    "        super().__init__()\n",
    "        self.tools = tools or SEOAgentTools()\n",
    "\n",
    "    # --- Observe chain ---\n",
    "\n",
    "    @start()\n",
    "    def Task_CollectGSC(self, state: SEOState) -> SEOState:\n",
    "        return node_collect_gsc(state, self.tools)\n",
    "\n",
    "    @listen(Task_CollectGSC)\n",
    "    def Task_CollectGA4(self, state: SEOState) -> SEOState:\n",
    "        return node_collect_ga4(state, self.tools)\n",
    "\n",
    "    @listen(Task_CollectGA4)\n",
    "    def Task_CollectSERP(self, state: SEOState) -> SEOState:\n",
    "        return node_collect_serp_snapshot(state, self.tools)\n",
    "\n",
    "    @listen(Task_CollectSERP)\n",
    "    def Task_CollectCrawlDiff(self, state: SEOState) -> SEOState:\n",
    "        return node_collect_crawl_diff(state, self.tools)\n",
    "\n",
    "    # --- Diagnose chain ---\n",
    "\n",
    "    @listen(Task_CollectCrawlDiff)\n",
    "    def Task_NormalizeInputs(self, state: SEOState) -> SEOState:\n",
    "        return node_normalize_inputs(state, self.tools)\n",
    "\n",
    "    @listen(Task_NormalizeInputs)\n",
    "    def Task_DetectAnomalies(self, state: SEOState) -> SEOState:\n",
    "        return node_detect_anomalies(state, self.tools)\n",
    "\n",
    "    @listen(Task_DetectAnomalies)\n",
    "    def Task_ClassifyIssues(self, state: SEOState) -> SEOState:\n",
    "        return node_classify_issues(state, self.tools)\n",
    "\n",
    "    # --- Recommend chain ---\n",
    "\n",
    "    @listen(Task_ClassifyIssues)\n",
    "    def Task_ScoreOpportunitiesAndBuildQueue(self, state: SEOState) -> SEOState:\n",
    "        return node_score_opportunities(state, self.tools)\n",
    "\n",
    "    @listen(Task_ScoreOpportunitiesAndBuildQueue)\n",
    "    def Task_RouteNextOpportunity(self, state: SEOState) -> SEOState:\n",
    "        # We call the same router node, then do simple branching\n",
    "        state = node_route_by_type_and_priority(state, self.tools)\n",
    "        next_node = _router_condition(state)\n",
    "        if next_node == \"techseo_agent_node\":\n",
    "            return node_techseo_agent_node(state, self.tools)\n",
    "        if next_node == \"productseo_agent_node\":\n",
    "            return node_productseo_agent_node(state, self.tools)\n",
    "        if next_node == \"content_agent_node\":\n",
    "            return node_content_agent_node(state, self.tools)\n",
    "        if next_node == \"authority_agent_node\":\n",
    "            return node_authority_agent_node(state, self.tools)\n",
    "        # If router says QA, skip to QA\n",
    "        return state\n",
    "\n",
    "    # --- QA / Approve / Execute / Validate chain ---\n",
    "\n",
    "    @listen(Task_RouteNextOpportunity)\n",
    "    def Task_QAGuardrail(self, state: SEOState) -> SEOState:\n",
    "        return node_qa_guardrail_node(state, self.tools)\n",
    "\n",
    "    @listen(Task_QAGuardrail)\n",
    "    def Task_ApprovalGate(self, state: SEOState) -> SEOState:\n",
    "        # Same logic as LangGraph conditional edges\n",
    "        decision = _qa_condition(state)\n",
    "        if decision == \"approval_interrupt_node\":\n",
    "            state = node_approval_interrupt_node(state, self.tools)\n",
    "        # After approval gate, we route again as in LangGraph\n",
    "        decision2 = _approval_gate_condition(state)\n",
    "        if decision2 == \"execute_changeset_node\":\n",
    "            state = node_execute_changeset_node(state, self.tools)\n",
    "        elif decision2 == \"create_tickets_node\":\n",
    "            state = node_create_tickets_node(state, self.tools)\n",
    "        return state\n",
    "\n",
    "    @listen(Task_ApprovalGate)\n",
    "    def Task_ValidateMetrics(self, state: SEOState) -> SEOState:\n",
    "        return node_validate_metrics_node(state, self.tools)\n",
    "\n",
    "    @listen(Task_ValidateMetrics)\n",
    "    def Task_PersistRun(self, state: SEOState) -> SEOState:\n",
    "        return node_persist_run_node(state, self.tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1d1cb",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# Step 6: Minimal smoke test / visualization hook\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59a4f7de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'START' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Implement: basic end-to-end invocation of the LangGraph app\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     tools \u001b[38;5;241m=\u001b[39m SEOAgentTools()\n\u001b[1;32m----> 4\u001b[0m     app \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_langgraph_universal_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     initial_state: SEOState \u001b[38;5;241m=\u001b[39m ensure_default_state(\n\u001b[0;32m      7\u001b[0m         SEOState(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m             run\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo_run_001\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aurumpickleball.com\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         )\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     final_state \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39minvoke(initial_state)\n",
      "Cell \u001b[1;32mIn[15], line 113\u001b[0m, in \u001b[0;36mbuild_langgraph_universal_app\u001b[1;34m(tools)\u001b[0m\n\u001b[0;32m    110\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_node(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersist_run_node\u001b[39m\u001b[38;5;124m\"\u001b[39m, bind(node_persist_run_node))\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Direct edges (Observe → Diagnose → Recommend → Router)\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_edge(\u001b[43mSTART\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect_gsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect_gsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect_ga4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    115\u001b[0m graph\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect_ga4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollect_serp_snapshot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'START' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Implement: basic end-to-end invocation of the LangGraph app\n",
    "    tools = SEOAgentTools()\n",
    "    app = build_langgraph_universal_app(tools)\n",
    "\n",
    "    initial_state: SEOState = ensure_default_state(\n",
    "        SEOState(  # type: ignore[call-arg]\n",
    "            run={\"run_id\": \"demo_run_001\", \"source\": \"manual\", \"domain\": \"https://aurumpickleball.com\"},\n",
    "            config={\n",
    "                \"risk\": {\n",
    "                    \"approval_threshold\": 4,\n",
    "                    \"auto_execute_max_risk\": 2,\n",
    "                    \"blocklist_kinds\": [],\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    final_state = app.invoke(initial_state)\n",
    "    print(\"Work queue:\", final_state[\"plan\"][\"work_queue\"])\n",
    "    print(\"Changesets:\", len(final_state[\"plan\"][\"changesets\"]))\n",
    "    print(\"Tickets:\", len(final_state[\"plan\"][\"tickets\"]))\n",
    "    print(\"Applied:\", final_state[\"execution\"][\"applied_changesets\"])\n",
    "    print(\"Audit events:\", len(final_state[\"audit_log\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f7028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
